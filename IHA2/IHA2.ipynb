{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad8b0feffaddac686389c323e8f7b862",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of points (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use a cloud GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"IHA2.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "270e43e75da54d7fb8afbda64083f4e3",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1 and HA2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Mattia Carlino\"\n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b2403e87a33f87371b150984248355",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "\n",
    "assert (\n",
    "    python_version_tuple()[:2] == (\"3\", \"11\")\n",
    "), \"You are not running Python 3.11. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c741699084b67aa21d06ff931465b378",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nb_dirname = os.path.abspath(\"\")\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in [\n",
    "    \"IHA1\",\n",
    "    \"IHA2\",\n",
    "    \"HA1\",\n",
    "    \"HA2\",\n",
    "], \"[ERROR] The notebook appears to have been moved from its original directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1709bd6d2b55a83969e44d70763b1167",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        HTML(\n",
    "            r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(\n",
    "                nb_fname=nb_fname\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "except NameError:\n",
    "    assert False, \"Make sure to fill in the nb_fname variable above!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f405c9cd7b9720915f79dba54c89375",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "587eb0d471650fb68cb3c1f5021df2d4",
     "grade": false,
     "grade_id": "cell-1420bd2a80dfa7d5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# IHA2 - Catching Pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57c5f7037fd0f1ca2377e5f021815405",
     "grade": false,
     "grade_id": "cell-c0cb4dde48293818",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/en/4/46/Pokemon_Go.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9acef9de3d4fc4337e7dadd4fd579462",
     "grade": false,
     "grade_id": "cell-b16d667dd74a9079",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this home assignment, you'll apply roughly the same principles we used when doing logistic regression on the Iris dataset in Computer Lab 1, but on a different and very interesting dataset. We'll use the [Predict'em All dataset from Kaggle](https://www.kaggle.com/semioniy/predictemall). To download the dataset you will need a Kaggle account. This dataset consists of roughly 293,000 [pokemon](http://www.pokemongo.com/) sightings (historical appearances of Pokemon in the Pokemon Go game), with geographical coordinates, time, weather, population density, distance to pokestops/gyms etc. as features. A comprehensive list of all the features is available at [the dataset's homepage](https://www.kaggle.com/semioniy/predictemall).\n",
    "\n",
    "The context is simple: you are a Pokemon hunter, and there are only three Pokemon left for you to complete your collection. You'll do anything to capture them, including changing where you'll spend your next holidays! You know that some Pokemon only spawn in certain places of the world. Since you like machine learning so much, you figure it would be a great idea to train a classifier that, based on a location's longitude and latitude, can tell us which Pokemon is more likely to appear there.\n",
    "\n",
    "The assignment is broken down into six steps.\n",
    "\n",
    "1. Loading the data and extracting the desired subset of it\n",
    "2. Visualization of the dataset\n",
    "3. Preprocessing\n",
    "4. Training\n",
    "5. Evaluation\n",
    "6. Exploration\n",
    "\n",
    "Feel free to temporarily add cells wherever you see fit, and play around with this notebook as much as you want when developing the solutions. However, the solution you upload to Canvas must have the exact format shown here, with only the cells present here.\n",
    "\n",
    "Don't restrict yourself only to what was taught so far. Some of the tasks might require you to search for new information. However, **be sure that you do the assignment using PyTorch** since we will be using it through the following assignments as well. [The Python docs](https://docs.python.org/3/), [PyTorch docs](https://pytorch.org/docs/stable/index.html), [stackoverflow](https://stackoverflow.com/), and Google are your friends!\n",
    "\n",
    "**Hint:** Solving Computer Lab 1 (CL1) is a good way to get prepared for this assignment.\n",
    "\n",
    "To pass this assignment, your solutions should pass all tests (`assert`-statements). Note that the tests shown to you are not exhaustive, and additional hidden tests exist for some of the tasks. Further, similar to IHA1, this notebook contains some questions where we ask you to reflect upon some results. These questions will not be graded in detail, but we still expect you to answer them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d8a3b20e9634f5c918019159cde8280",
     "grade": false,
     "grade_id": "cell-b9e4c4aa45490941",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca68d5a34a1d6f8633072976fad132a3",
     "grade": false,
     "grade_id": "cell-548d168c5c9e8c39",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Import any necessary modules here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bd0c1d994ba501cd83cb86c40a43122",
     "grade": false,
     "grade_id": "cell-4e61b7fa879ef4a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a576cdb7f1a3658a7eb906977c5e40fb",
     "grade": false,
     "grade_id": "cell-50c33a3517aea662",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1. Loading and extracting subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f1178610f2a3de2652d78bf724e5c6a",
     "grade": false,
     "grade_id": "cell-32f6c44c22e84d42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first step consists of filtering the dataset by the three Pokémon you are interested at. \n",
    "\n",
    "Start by loading the `'300k.csv'` file using pandas. If you haven't downloaded it yet, either use [this link](https://www.kaggle.com/semioniy/predictemall) to do so (and place the file in the same folder as this notebook), or simply run the cell below. You might have to [create a new API token](https://www.kaggle.com/settings/account) before the commands work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8b85c109b1e96fc2c692b69d07573ea",
     "grade": false,
     "grade_id": "cell-39c13080b772bfe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d semioniy/predictemall\n",
    "!unzip -u predictemall.zip\n",
    "!rm -rf predictemall.zip 300k_arff 300k.arff 300k_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1308d475120c2d9e46b37129e7221f99",
     "grade": false,
     "grade_id": "cell-53b2a6c0754fca1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: load the dataset using pandas to a dataframe called df\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_csv('300k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "937dc09867c39ed4282c3552a0a2d3fc",
     "grade": true,
     "grade_id": "cell-d7bd8d78c4388b4d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df.shape == (\n",
    "    296021,\n",
    "    208,\n",
    "), f\"Dataframe has not the right shape. {df.shape} != (296021, 208)\"\n",
    "assert isinstance(df, pd.DataFrame), f\"df is not a dataframe. Was {type(df)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e5a6ed587a81abc562131a9a019da9a",
     "grade": false,
     "grade_id": "cell-5d36e8f9780ea946",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Modify `df` to only have the columns `latitude`, `longitude`, and `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32fe82a12426d7fd89c7b44803e9901d",
     "grade": false,
     "grade_id": "cell-318e20fd02ab7f30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df = df[['latitude', 'longitude', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "334ba32ebb98cfbe793e8d06ee8d3b34",
     "grade": true,
     "grade_id": "cell-e8738beea1b331d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(df.columns) == 3, \"There should be 3 columns\"\n",
    "assert len(df.shape) == 2, \"The dataframe should be 2 dimensional\"\n",
    "assert df.shape == (296021, 3), \"Wrong shape of the dataframe\"\n",
    "assert \"latitude\" in df.columns, \"latitude column is missing\"\n",
    "assert \"longitude\" in df.columns, \"longitude column is missing\"\n",
    "assert \"class\" in df.columns, \"class column is missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ef5470f58caaaea609c2fee299c5b7f",
     "grade": false,
     "grade_id": "cell-6f7970ff67af4649",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that the `class` column specifies which pokemon it is. However, it only has the numerical id of the pokemon. For your convenience, we provide the dictionary `name_dict` to convert between ids and names (we'll do this soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e1c295d53b9c78e248276d093849a02",
     "grade": false,
     "grade_id": "cell-50ff16afeaf933cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bae8f433b0632b5da39f4d675f7f9db",
     "grade": false,
     "grade_id": "cell-1acfd7cca7bf754e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# example usage (you can index either by name or id)\n",
    "print(name_dict[\"Gengar\"])\n",
    "print(name_dict[94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c4b53d1aadb72239ba22773f6cdace6",
     "grade": false,
     "grade_id": "cell-1fd0d8b0c9800de4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# for convenience, let's add a new column to the dataframe with the name of the pokemon\n",
    "df[\"name\"] = df[\"class\"].apply(lambda x: name_dict[x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09025bb17628db2a36bb21a5b3b6f1c5",
     "grade": false,
     "grade_id": "cell-eaa874b93b3b727c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We are only interested in three specific pokemon: Diglett, Seel, and Tauros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca357363a50da71ade1d068b235fbd63",
     "grade": false,
     "grade_id": "cell-7e0898a53c5649b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th> <center>Diglett</center> </th>\n",
    "    <th> <center>Seel</center> </th> \n",
    "    <th> <center>Tauros</center> </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=https://assets.pokemon.com/assets/cms2/img/pokedex/full/050_f2.png alt=Digglet></td>\n",
    "    <td><img src=https://pokemon.gamepedia.com/media/pokemon.gamepedia.com/thumb/f/f1/Seel.png/200px-Seel.png?version=2c32fbe0af2d0da707e5dbcb40472fbf></td>\n",
    "    <td><img src=https://www.pokemon.com/static-assets/content-assets/cms2/img/pokedex/full/128.png></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f67b745d8f97bcaf04b6674bab71e2d",
     "grade": false,
     "grade_id": "cell-d091927bf7d7938f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Filter the dataset to contain only these three types of pokemon and save it in the DataFrame `filtered_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3101497237b105c936f9b72972e4e06f",
     "grade": false,
     "grade_id": "cell-7089f3397cbc1f4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "filtered_df = df.query('name == \"Diglett\" or name == \"Seel\" or name == \"Tauros\"')\n",
    "print(\n",
    "    f\"We have {len(filtered_df)} instances of Diglett, Seel, and Tauros in the dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "804e5464b130a64d22b06d0b882d0c89",
     "grade": true,
     "grade_id": "cell-c24e683b633a9f9b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(np.unique(filtered_df[\"class\"])) == 3, \"There should be 3 unique classes.\"\n",
    "assert filtered_df.shape == (\n",
    "    2083,\n",
    "    4,\n",
    "), \"The shape of the filtered dataframe is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b1f1246460c9e81025c5dd94d54e9e2",
     "grade": false,
     "grade_id": "cell-4e7ef21d67cb017e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In an earlier cell, you could see that the dataset has 208 features per pokemon sighting (`df.shape == (296021, 208)`). Why do we only use the `longitude` and `latitude` features and not all the features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "345b05134d46ff03c9a71123906636f2",
     "grade": true,
     "grade_id": "cell-21cf8cbbffef4929",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** Because we are interested where pokemons can appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2641774ea6d909844fb9d2e930763382",
     "grade": false,
     "grade_id": "cell-01b988bb1bfa1f92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "437148ed483b9f34a3cd87aae3129933",
     "grade": false,
     "grade_id": "cell-5c7b8fdcd18575fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The second step consists of visualizing the dataset. This will help you understand the distribution of the features and get an idea of how hard the task will be.\n",
    "\n",
    "Plot a bar chart of the number of occurrences of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b6048c83c3f3f72c51e4baa47b141b",
     "grade": true,
     "grade_id": "cell-c1a9921488e42992",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "fig, ax = plt.subplots()\n",
    "pokemons = ['Diglett', 'Seel', 'Tauros']\n",
    "occurences = filtered_df['class'].value_counts()\n",
    "\n",
    "bar_labels = ['Diglett', 'Seel', 'Tauros']\n",
    "bar_colors = ['tab:red', 'tab:blue', 'tab:orange']\n",
    "\n",
    "ax.bar(pokemons, occurences, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('pokemon occurences')\n",
    "ax.set_title('Number of occurences of each class')\n",
    "ax.legend(title='Pokemon class')\n",
    "\n",
    "print('occ:', occurences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4c3c67ecc7c5b85749d001b0a622061",
     "grade": false,
     "grade_id": "cell-c22d835bdc58fe68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Is the dataset balanced? Why/why not? Why is this important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c918f9b5d190f5f83c8dbd622e59d062",
     "grade": true,
     "grade_id": "cell-3d060bda26842b5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "\n",
    "**Your answer:** The dataset is not exactly balanced, since each class has different occurences. Balanced Datasets ensure that the model treats all classes equally, allowing it to learn features from all classes more effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4382ce93389defee634b029f25edda16",
     "grade": false,
     "grade_id": "cell-931cbd9e3ec95da5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot a scatter plot where the first dimension is longitude, the second is latitude, and each point is a Pokemon. Further, the color of each point should represent which Pokemon it is. Lastly, the marker at each point should be an `'x'`. Make sure to label each axis.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- The `scatter` method from `matplotlib` accepts an argument called `c`.\n",
    "- The `scatter` method also accepts an argument called `marker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6f749d2605a9f0d0ae2dff5d584d414",
     "grade": true,
     "grade_id": "cell-1120bd5aa8abeae1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "colors = {'Diglett': '#ff9999',\n",
    "          'Seel': '#99ccff',   \n",
    "          'Tauros': '#ffcc99'}\n",
    "for class_name, class_df_filtered in filtered_df.groupby('name'):\n",
    "    plt.scatter(class_df_filtered['longitude'], class_df_filtered['latitude'], c=colors[class_name], label=class_name, marker='x')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Scatter plot of Pokémon by Location')\n",
    "plt.legend(title='Pokémon Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cad292133d28b4b0fe7f2245b0bc567",
     "grade": false,
     "grade_id": "cell-76f326a05bf22e09",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Is there any other visualization you think would be useful? If so, insert them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad81f10f7f24ca21197794fdcd7fb04c",
     "grade": true,
     "grade_id": "cell-86724bfd3955c5ed",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6140988fa08ca186bbe62a74e82ddddd",
     "grade": false,
     "grade_id": "cell-7af3ac7849dc5252",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How hard do you think the problem is? Which classes can/cannot be easily separated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bb25f67d0cf2e55d60f2b63646ce9fe",
     "grade": true,
     "grade_id": "cell-62e50deb2cca74b2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** From the scatter plot, it seems like there are clusters of Pokemon classes in different regions. Seel and Diglett appear to be the easiest to separate as them points cluster distinctly from the others. However, Diglett and Tauros have overlapping regions, particularly in the top-left area of the plot, making them harder to distinguish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e6bfdd82abab0b61a471c52bfc9dff3",
     "grade": false,
     "grade_id": "cell-3d322369934aa289",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Which accuracy do you expect to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "931b0e510dcc8867fb3efb7e50fe8a25",
     "grade": true,
     "grade_id": "cell-f2c255ad5dd7e5fc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Due to the clear separation between Diglett and Seel clusters, we can expect to achieve high general accuracy. However, the overlapping area with Tauros may slightly can reduce the specific class accuracy. A reasonable estimate would be around 70-80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5510f4a800380ecc156cd9d2f3898e5",
     "grade": false,
     "grade_id": "cell-451e0813fca28b8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a6c54a6e0bfe71ad1da3f834fbcb3d7",
     "grade": false,
     "grade_id": "cell-3eed8190495b60ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The third step consists of processing the data before training, such as dividing the dataset into training, validation, and test sets. Some tranformations can also be applied to the dataset in order to improve the performance of the network. We will use some PyTorch utilities to help us with this task.\n",
    "\n",
    "Start by creating the input and output vectors, `x` and `y`. The input should be latitude and longitude, and the output should be the class of the pokemon. Note that you cannot use the class names directly, as they are strings. You need to introduce some mapping to convert them to integers (0, 1, and 2) or one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a001f72e92feb0435ca66ca282c5eb6c",
     "grade": false,
     "grade_id": "cell-c61cb8065babf0c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Input vector\n",
    "# Use the latitude and longitude as input features\n",
    "x = filtered_df[['latitude', 'longitude']].to_numpy(dtype=\"float32\")\n",
    "\n",
    "#Output vector\n",
    "pokemon_mapping = {'Diglett': 0, 'Seel': 1, 'Tauros': 2}\n",
    "# Label encoding\n",
    "y = filtered_df['name'].apply(lambda name: pokemon_mapping[name]).to_numpy()\n",
    "\n",
    "print(f\"Shape of input data: {x.shape}\")\n",
    "print(f\"Shape of labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c29016eeb4001be249e61f5364c9c294",
     "grade": true,
     "grade_id": "cell-e3cfcf9955f6809c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(x, np.ndarray), \"x should be a numpy array\"\n",
    "assert isinstance(y, np.ndarray), \"y should be a numpy array\"\n",
    "\n",
    "assert x.shape[0] == y.shape[0], \"x and y should have the same number of samples\"\n",
    "assert x.shape[-1] == 2, \"x should have 2 features\"\n",
    "assert x.dtype == np.float32, \"x should be of type float32\"\n",
    "if y.shape[-1] == 3:  # one-hot encoded\n",
    "    assert y.max() == 1, \"one-hot encoded y, at least one entry should be 1\"\n",
    "    assert y.min() == 0, \"one-hot encoded y, at least one entry should be 0\"\n",
    "    assert y.sum(axis=1).all() == 1\n",
    "else:  # label encoded\n",
    "    assert y.max() == 2, \"label encoded y, should have a max value of 2\"\n",
    "    assert y.min() == 0, \"label encoded y, should have a min value of 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "667ef8d517475a54aca21e510c96761d",
     "grade": false,
     "grade_id": "cell-566d2f340fa57d49",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Separate your data into training (55%), validation (25%) and test sets (20%) and save them as `dataset_train`, `dataset_val`, `dataset_test`. If you wish to apply any transformation to the dataset, do it here as well. \n",
    "\n",
    "Further, create a class, PokemonDataset, inheriting from PyTorch [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and use this for storing the data. In other words, `dataset_train`, `dataset_val`, `dataset_test` should have type PokemonDataset. You will need to implement a `__getitem__`, `__len__` and `__init__` method. Although perhaps a bit overkill for this assignment, it is a good practice for handling datasets in PyTorch.\n",
    "\n",
    "Last, instantiate a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for each dataset, i.e., `loader_train`, `loader_val`, `loader_test`. This will fetch samples from the datasets and combine them into batches. Remember to select a suitable batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2a4956e2e49a791cfcf70933886dacd",
     "grade": false,
     "grade_id": "cell-918e3bb4d74472bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# separate dataset into train-val and test set\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# separate into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split (x_train_val, y_train_val, test_size=0.3125, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(x_train)}\")\n",
    "print(f\"Validation set size: {len(x_val)}\")\n",
    "print(f\"Test set size: {len(x_test)}\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Create our own Pokemon dataset\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'features': torch.tensor(self.x_data[idx], dtype=torch.float32),\n",
    "              'label': torch.tensor(self.y_data[idx], dtype=torch.long)}\n",
    "        return sample['features'], sample['label']\n",
    "\n",
    "# Create the datasets    \n",
    "dataset_train = PokemonDataset(x_train_scaled, y_train)\n",
    "dataset_val = PokemonDataset(x_val_scaled, y_val)\n",
    "dataset_test = PokemonDataset(x_test_scaled, y_test)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 512\n",
    "\n",
    "# Dataloader creation (datasets loaded into DataLoader)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "679946ba2a8db7542fb0a009ebdec14b",
     "grade": true,
     "grade_id": "cell-26a6a53ab59cb075",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(\n",
    "    dataset_train, PokemonDataset\n",
    "), \"dataset_train should be an instance of PokemonDataset\"\n",
    "assert isinstance(\n",
    "    dataset_val, PokemonDataset\n",
    "), \"dataset_val should be an instance of PokemonDataset\"\n",
    "assert isinstance(\n",
    "    dataset_test, PokemonDataset\n",
    "), \"dataset_test should be an instance of PokemonDataset\"\n",
    "\n",
    "assert (\n",
    "    abs(len(dataset_train) / len(x) - 0.55) < 0.01\n",
    "), \"dataset_train has the wrong length, should be 55% of the data\"\n",
    "assert (\n",
    "    abs(len(dataset_val) / len(x) - 0.25) < 0.01\n",
    "), \"dataset_val has the wrong length, should be 25% of the data\"\n",
    "assert (\n",
    "    abs(len(dataset_test) / len(x) - 0.20) < 0.01\n",
    "), \"dataset_test has the wrong length, should be 20% of the data\"\n",
    "\n",
    "assert isinstance(\n",
    "    loader_train, DataLoader\n",
    "), \"loader_train should be an instance of DataLoader\"\n",
    "assert isinstance(\n",
    "    loader_val, DataLoader\n",
    "), \"loader_val should be an instance of DataLoader\"\n",
    "assert isinstance(\n",
    "    loader_test, DataLoader\n",
    "), \"loader_test should be an instance of DataLoader\"\n",
    "\n",
    "assert len(loader_train), \"loader_train should have a length\"\n",
    "assert len(loader_val), \"loader_val should have a length\"\n",
    "assert len(loader_test), \"loader_test should have a length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9b5ffa094240411cb43919a003ba914",
     "grade": false,
     "grade_id": "cell-b9088197f0ab661c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c5d33400a0aa71b9c81a01181b7cb3b",
     "grade": false,
     "grade_id": "cell-1c4e4c6b7186c418",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The fourth step is where you will choose the architecture of your network (number of hidden layers, activation functions, etc.), optimizer, loss function and then train the network. \n",
    "\n",
    "Start by implementing a training loop, and a helper function to calculate the accuracy. The training loop should calculate the loss and accuracy for both the training and validation set and print it with some regular interval (each epoch, or every few epochs, for instance). It can also be helpful to plot the loss and accuracy for both the training and validation set, either in the training loop, or after it has finished (i.e., you have to store the values during training and return them).\n",
    "\n",
    "We have prepared the `train_model` function with the arguments you need, but you have access to *args and **kwargs if you want to pass additional arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "651fa27aab4bd856b89ccdb51201cdf9",
     "grade": false,
     "grade_id": "cell-e1cb09a6dcbdb401",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "\n",
    "\n",
    "def accuracy(y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "    y_hat: torch.Tensor: The model predictions (probability per class), shape: [batch_size, num_classes]\n",
    "    y: torch.Tensor: The true labels, shape: [batch_size]\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The accuracy of the model\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Get the predicted class by taking the argmax over the class dimension\n",
    "    prediction = torch.argmax(y_hat, dim=1)\n",
    "    # Compute accuracy by comparing predictions with true labels\n",
    "    acc = torch.tensor(prediction == y).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim,\n",
    "    loss_fn: Union[Callable, nn.Module],\n",
    "    num_epochs: int,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "\n",
    "    Args:\n",
    "    model: nn.Module: The neural network model\n",
    "    optimizer: torch.optim: The optimizer used to update the model parameters\n",
    "    loss_fn: Union[Callable, nn.Module]: The loss function used to compute the loss\n",
    "    num_epochs: int: The number of epochs to train the model\n",
    "    train_dataloader: torch.utils.data.DataLoader: The training dataloader\n",
    "    val_dataloader: torch.utils.data.DataLoader: The validation dataloader\n",
    "    device: Union[str, torch.device]: The device to run the training on\n",
    "    *args: Additional arguments to pass to the train function\n",
    "    **kwargs: Additional keyword arguments to pass to the train function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Set the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Iterate over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize running loss and accuracy for the current epoch\n",
    "        current_train_loss = 0\n",
    "        current_train_acc = 0\n",
    "        \n",
    "        for input_batch, labels_batch in train_dataloader:\n",
    "            # Move the batch to the device\n",
    "            input_batch, labels_batch = input_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "            # Zero the parameter gradients, to avoid accumulation of gradients in the optimizer \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute forward pass (compute predictions)\n",
    "            outputs = model(input_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training loss and accuracy\n",
    "            current_train_loss += loss.item()\n",
    "            current_train_acc += accuracy(outputs, labels_batch).item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        current_train_loss /= len(train_dataloader)\n",
    "        current_train_acc /= len(train_dataloader)\n",
    "        train_losses.append(current_train_loss)\n",
    "        train_accuracies.append(current_train_acc)\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        current_val_loss = 0\n",
    "        current_val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs_batch, labels_batch in val_dataloader:\n",
    "                # Move data to device\n",
    "                inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs_batch)\n",
    "                loss = loss_fn(outputs, labels_batch)\n",
    "                \n",
    "                # Update validation loss and accuracy\n",
    "                current_val_loss += loss.item()\n",
    "                current_val_acc += accuracy(outputs, labels_batch).item()\n",
    "\n",
    "        # Average validation loss and accuracy for the epoch\n",
    "        current_val_loss /= len(val_dataloader)\n",
    "        current_val_acc /= len(val_dataloader)\n",
    "        val_losses.append(current_val_loss)\n",
    "        val_accuracies.append(current_val_loss)\n",
    "\n",
    "        # Print loss and accuracy\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {current_train_loss:.4f}, Train Acc: {current_train_acc:.4f}, \"\n",
    "              f\"Val Loss: {current_train_loss:.4f}, Val Acc: {current_val_acc:.4f}\")\n",
    "\n",
    "    # Return the losses and accuracies\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f120e556ca9bee3969022ccd95c884d5",
     "grade": false,
     "grade_id": "cell-abd73fe36efa945d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we'll test that your training loop is correct. A [common sanity check in deep learning](https://karpathy.github.io/2019/04/25/recipe/) is to overfit to a small dataset, like a single batch of data. This ensures that shapes and devices are correctly set, and the network can learn/memorize the training data, which is a good starting point before training on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4c52b6973df138e3fbe8f0ce30b32d",
     "grade": true,
     "grade_id": "cell-be96e21b4495aa27",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_train_loop():\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data as data\n",
    "\n",
    "    # init simple model, optimizer, loss_fn, dataloaders\n",
    "    linear_model = nn.Linear(2, 3)\n",
    "    optimizer = optim.SGD(linear_model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    testing_loader_train = data.DataLoader(\n",
    "        data.TensorDataset(torch.randn(2, 2), torch.randint(0, 3, (2,)))\n",
    "    )\n",
    "    testing_loader_val = data.DataLoader(\n",
    "        data.TensorDataset(torch.randn(2, 2), torch.randint(0, 3, (2,)))\n",
    "    )\n",
    "\n",
    "    # copy parameters to check for changes\n",
    "    params_before_training = list(linear_model.parameters())\n",
    "    params_before_training = [p.clone().to(\"cpu\") for p in params_before_training]\n",
    "\n",
    "    print(\"Testing training loop, CPU\")\n",
    "    train_model(\n",
    "        model=linear_model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        num_epochs=100,\n",
    "        train_dataloader=testing_loader_train,\n",
    "        val_dataloader=testing_loader_val,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Testing training loop, CUDA\")\n",
    "        train_model(\n",
    "            model=linear_model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            num_epochs=100,\n",
    "            train_dataloader=testing_loader_train,\n",
    "            val_dataloader=testing_loader_val,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "\n",
    "    params_after_training = list(linear_model.parameters())\n",
    "    params_after_training = [p.clone().to(\"cpu\") for p in params_after_training]\n",
    "    for p_before, p_after in zip(params_before_training, params_after_training):\n",
    "        assert torch.any(\n",
    "            p_before != p_after\n",
    "        ), \"Model parameters did not change during training\"\n",
    "\n",
    "    # check that we could overfit a single example\n",
    "    (x, y) = next(iter(testing_loader_train))\n",
    "    x.to(\"cpu\")\n",
    "    y.to(\"cpu\")\n",
    "    linear_model.to(\"cpu\")\n",
    "    assert (\n",
    "        linear_model(x).argmax().item() == y.item()\n",
    "    ), \"Model could not overfit a single example\"\n",
    "\n",
    "\n",
    "def test_accuracy():\n",
    "    y_pred = torch.tensor([[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.1, 0.1, 0.8]])\n",
    "    y_true = torch.tensor([1, 0, 2])\n",
    "    acc = accuracy(y_pred, y_true)\n",
    "    assert isinstance(acc, torch.Tensor), \"Accuracy should be a torch.Tensor\"\n",
    "    assert torch.isclose(acc, torch.tensor(2 / 3)), f\"Accuracy is {acc}, expected 2/3\"\n",
    "\n",
    "\n",
    "try:\n",
    "    test_train_loop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    assert False, \"Training test failed, see error above\"\n",
    "\n",
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd84899c109e58d0664fc4fab26425a2",
     "grade": false,
     "grade_id": "cell-a13a4a8eef1c582f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, create a neural network using PyTorch. You can use any architecture you want. Save the model in the variable `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccbab415641442556c01ed5667f4ea77",
     "grade": false,
     "grade_id": "cell-dff32f0d1dd91a72",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# A neural network model with 4 fully connected layers and batch normalization after each layer \n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Initialization of the layers in the model\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)    \n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization for the first layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)   # Batch normalization for the second layer\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)   # Batch normalization for the third layer\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)  # Apply batch normalization\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = torch.softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model (input_size=2 for longitude and latitude, num_classes=3 for the 3 pokemon classes)\n",
    "model = NeuralNetwork(input_size=2, num_classes=3)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d10331a48b8b11d2d2056d610b13b7f",
     "grade": true,
     "grade_id": "cell-a3f55091f5cbbdfe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(model, nn.Module)\n",
    "\n",
    "\n",
    "# check that we can run input of shape (batch_size, 2) through the model, with batch_size=16\n",
    "def test_model_output_shape(model):\n",
    "    is_model_on_cuda = next(model.parameters()).is_cuda\n",
    "    device = torch.device(\"cuda\" if is_model_on_cuda else \"cpu\")\n",
    "    assert (\n",
    "        len(model(torch.randn(16, 2, device=device))) == 16\n",
    "    ), \"The model should not change the batch size\"\n",
    "\n",
    "\n",
    "test_model_output_shape(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de0dda92011c512396593caba091db79",
     "grade": false,
     "grade_id": "cell-d91e1230c6b9225d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train the network. \n",
    "\n",
    "For you to pass this assignment, you must obtain an accuracy on the test set greater than 60%. You can use the validation set as a proxy during development, but remember that they can differ slightly. We use the test set, as this better represents the performance of the model on new, unseen data (which is the ultimate goal).\n",
    "\n",
    "To reach the level of performance, it may be necessary to search for a good architecture by trying several different ones. Last, if you want a challenge, try getting an accuracy greater than 75% (our reference solution achieves ~78%).\n",
    "\n",
    "Again, it might be useful to plot the loss and accuracy (for training and validation) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "335efaf08cc5e4593fe97659a126cd84",
     "grade": false,
     "grade_id": "cell-711ca3d58ad725b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/n9y0bc4n5zgb8gg118j_j66h0000gn/T/ipykernel_15008/46787403.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.tensor(prediction == y).float().mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.0728, Train Acc: 0.5308, Val Loss: 1.0728, Val Acc: 0.3190\n",
      "Epoch [2/200], Train Loss: 1.0010, Train Acc: 0.6573, Val Loss: 1.0010, Val Acc: 0.4362\n",
      "Epoch [3/200], Train Loss: 0.9686, Train Acc: 0.6967, Val Loss: 0.9686, Val Acc: 0.6292\n",
      "Epoch [4/200], Train Loss: 0.9488, Train Acc: 0.6743, Val Loss: 0.9488, Val Acc: 0.6439\n",
      "Epoch [5/200], Train Loss: 0.9391, Train Acc: 0.6958, Val Loss: 0.9391, Val Acc: 0.5883\n",
      "Epoch [6/200], Train Loss: 0.9201, Train Acc: 0.6959, Val Loss: 0.9201, Val Acc: 0.6020\n",
      "Epoch [7/200], Train Loss: 0.9151, Train Acc: 0.7015, Val Loss: 0.9151, Val Acc: 0.6722\n",
      "Epoch [8/200], Train Loss: 0.9095, Train Acc: 0.7010, Val Loss: 0.9095, Val Acc: 0.6790\n",
      "Epoch [9/200], Train Loss: 0.8976, Train Acc: 0.7225, Val Loss: 0.8976, Val Acc: 0.6790\n",
      "Epoch [10/200], Train Loss: 0.8824, Train Acc: 0.7225, Val Loss: 0.8824, Val Acc: 0.6761\n",
      "Epoch [11/200], Train Loss: 0.8883, Train Acc: 0.7206, Val Loss: 0.8883, Val Acc: 0.6751\n",
      "Epoch [12/200], Train Loss: 0.8789, Train Acc: 0.7183, Val Loss: 0.8789, Val Acc: 0.6810\n",
      "Epoch [13/200], Train Loss: 0.8780, Train Acc: 0.7223, Val Loss: 0.8780, Val Acc: 0.6820\n",
      "Epoch [14/200], Train Loss: 0.8647, Train Acc: 0.7328, Val Loss: 0.8647, Val Acc: 0.6810\n",
      "Epoch [15/200], Train Loss: 0.8651, Train Acc: 0.7301, Val Loss: 0.8651, Val Acc: 0.6820\n",
      "Epoch [16/200], Train Loss: 0.8773, Train Acc: 0.7110, Val Loss: 0.8773, Val Acc: 0.6820\n",
      "Epoch [17/200], Train Loss: 0.8532, Train Acc: 0.7404, Val Loss: 0.8532, Val Acc: 0.6820\n",
      "Epoch [18/200], Train Loss: 0.8482, Train Acc: 0.7393, Val Loss: 0.8482, Val Acc: 0.6810\n",
      "Epoch [19/200], Train Loss: 0.8389, Train Acc: 0.7543, Val Loss: 0.8389, Val Acc: 0.6829\n",
      "Epoch [20/200], Train Loss: 0.8546, Train Acc: 0.7176, Val Loss: 0.8546, Val Acc: 0.6839\n",
      "Epoch [21/200], Train Loss: 0.8424, Train Acc: 0.7365, Val Loss: 0.8424, Val Acc: 0.6859\n",
      "Epoch [22/200], Train Loss: 0.8469, Train Acc: 0.7253, Val Loss: 0.8469, Val Acc: 0.6829\n",
      "Epoch [23/200], Train Loss: 0.8514, Train Acc: 0.7270, Val Loss: 0.8514, Val Acc: 0.6849\n",
      "Epoch [24/200], Train Loss: 0.8496, Train Acc: 0.7239, Val Loss: 0.8496, Val Acc: 0.6868\n",
      "Epoch [25/200], Train Loss: 0.8599, Train Acc: 0.7099, Val Loss: 0.8599, Val Acc: 0.6888\n",
      "Epoch [26/200], Train Loss: 0.8376, Train Acc: 0.7337, Val Loss: 0.8376, Val Acc: 0.6888\n",
      "Epoch [27/200], Train Loss: 0.8320, Train Acc: 0.7434, Val Loss: 0.8320, Val Acc: 0.6868\n",
      "Epoch [28/200], Train Loss: 0.8410, Train Acc: 0.7301, Val Loss: 0.8410, Val Acc: 0.6849\n",
      "Epoch [29/200], Train Loss: 0.8270, Train Acc: 0.7463, Val Loss: 0.8270, Val Acc: 0.6859\n",
      "Epoch [30/200], Train Loss: 0.8280, Train Acc: 0.7427, Val Loss: 0.8280, Val Acc: 0.6859\n",
      "Epoch [31/200], Train Loss: 0.8239, Train Acc: 0.7497, Val Loss: 0.8239, Val Acc: 0.6849\n",
      "Epoch [32/200], Train Loss: 0.8125, Train Acc: 0.7612, Val Loss: 0.8125, Val Acc: 0.6878\n",
      "Epoch [33/200], Train Loss: 0.8279, Train Acc: 0.7455, Val Loss: 0.8279, Val Acc: 0.6859\n",
      "Epoch [34/200], Train Loss: 0.8335, Train Acc: 0.7392, Val Loss: 0.8335, Val Acc: 0.6849\n",
      "Epoch [35/200], Train Loss: 0.8437, Train Acc: 0.7201, Val Loss: 0.8437, Val Acc: 0.6839\n",
      "Epoch [36/200], Train Loss: 0.8315, Train Acc: 0.7311, Val Loss: 0.8315, Val Acc: 0.6849\n",
      "Epoch [37/200], Train Loss: 0.8191, Train Acc: 0.7484, Val Loss: 0.8191, Val Acc: 0.6888\n",
      "Epoch [38/200], Train Loss: 0.8195, Train Acc: 0.7434, Val Loss: 0.8195, Val Acc: 0.6908\n",
      "Epoch [39/200], Train Loss: 0.8095, Train Acc: 0.7567, Val Loss: 0.8095, Val Acc: 0.6878\n",
      "Epoch [40/200], Train Loss: 0.8273, Train Acc: 0.7356, Val Loss: 0.8273, Val Acc: 0.6898\n",
      "Epoch [41/200], Train Loss: 0.8107, Train Acc: 0.7482, Val Loss: 0.8107, Val Acc: 0.6898\n",
      "Epoch [42/200], Train Loss: 0.8052, Train Acc: 0.7526, Val Loss: 0.8052, Val Acc: 0.6888\n",
      "Epoch [43/200], Train Loss: 0.8241, Train Acc: 0.7363, Val Loss: 0.8241, Val Acc: 0.6849\n",
      "Epoch [44/200], Train Loss: 0.8100, Train Acc: 0.7557, Val Loss: 0.8100, Val Acc: 0.6868\n",
      "Epoch [45/200], Train Loss: 0.8110, Train Acc: 0.7549, Val Loss: 0.8110, Val Acc: 0.6888\n",
      "Epoch [46/200], Train Loss: 0.7979, Train Acc: 0.7643, Val Loss: 0.7979, Val Acc: 0.6888\n",
      "Epoch [47/200], Train Loss: 0.7991, Train Acc: 0.7622, Val Loss: 0.7991, Val Acc: 0.6888\n",
      "Epoch [48/200], Train Loss: 0.8092, Train Acc: 0.7585, Val Loss: 0.8092, Val Acc: 0.6888\n",
      "Epoch [49/200], Train Loss: 0.8102, Train Acc: 0.7542, Val Loss: 0.8102, Val Acc: 0.6908\n",
      "Epoch [50/200], Train Loss: 0.8272, Train Acc: 0.7350, Val Loss: 0.8272, Val Acc: 0.6908\n",
      "Epoch [51/200], Train Loss: 0.8006, Train Acc: 0.7687, Val Loss: 0.8006, Val Acc: 0.6927\n",
      "Epoch [52/200], Train Loss: 0.8057, Train Acc: 0.7568, Val Loss: 0.8057, Val Acc: 0.6917\n",
      "Epoch [53/200], Train Loss: 0.8247, Train Acc: 0.7418, Val Loss: 0.8247, Val Acc: 0.6888\n",
      "Epoch [54/200], Train Loss: 0.8173, Train Acc: 0.7564, Val Loss: 0.8173, Val Acc: 0.6898\n",
      "Epoch [55/200], Train Loss: 0.8152, Train Acc: 0.7444, Val Loss: 0.8152, Val Acc: 0.6908\n",
      "Epoch [56/200], Train Loss: 0.8077, Train Acc: 0.7565, Val Loss: 0.8077, Val Acc: 0.6908\n",
      "Epoch [57/200], Train Loss: 0.8061, Train Acc: 0.7482, Val Loss: 0.8061, Val Acc: 0.6888\n",
      "Epoch [58/200], Train Loss: 0.8010, Train Acc: 0.7724, Val Loss: 0.8010, Val Acc: 0.6859\n",
      "Epoch [59/200], Train Loss: 0.8079, Train Acc: 0.7499, Val Loss: 0.8079, Val Acc: 0.6888\n",
      "Epoch [60/200], Train Loss: 0.7894, Train Acc: 0.7730, Val Loss: 0.7894, Val Acc: 0.6898\n",
      "Epoch [61/200], Train Loss: 0.7969, Train Acc: 0.7695, Val Loss: 0.7969, Val Acc: 0.6908\n",
      "Epoch [62/200], Train Loss: 0.8111, Train Acc: 0.7458, Val Loss: 0.8111, Val Acc: 0.6888\n",
      "Epoch [63/200], Train Loss: 0.7965, Train Acc: 0.7593, Val Loss: 0.7965, Val Acc: 0.6878\n",
      "Epoch [64/200], Train Loss: 0.7958, Train Acc: 0.7693, Val Loss: 0.7958, Val Acc: 0.6956\n",
      "Epoch [65/200], Train Loss: 0.7998, Train Acc: 0.7651, Val Loss: 0.7998, Val Acc: 0.6908\n",
      "Epoch [66/200], Train Loss: 0.7995, Train Acc: 0.7591, Val Loss: 0.7995, Val Acc: 0.6908\n",
      "Epoch [67/200], Train Loss: 0.8206, Train Acc: 0.7397, Val Loss: 0.8206, Val Acc: 0.6898\n",
      "Epoch [68/200], Train Loss: 0.8181, Train Acc: 0.7444, Val Loss: 0.8181, Val Acc: 0.6868\n",
      "Epoch [69/200], Train Loss: 0.8048, Train Acc: 0.7555, Val Loss: 0.8048, Val Acc: 0.6868\n",
      "Epoch [70/200], Train Loss: 0.8025, Train Acc: 0.7541, Val Loss: 0.8025, Val Acc: 0.6859\n",
      "Epoch [71/200], Train Loss: 0.8085, Train Acc: 0.7513, Val Loss: 0.8085, Val Acc: 0.6849\n",
      "Epoch [72/200], Train Loss: 0.8004, Train Acc: 0.7588, Val Loss: 0.8004, Val Acc: 0.6878\n",
      "Epoch [73/200], Train Loss: 0.8048, Train Acc: 0.7507, Val Loss: 0.8048, Val Acc: 0.6878\n",
      "Epoch [74/200], Train Loss: 0.8016, Train Acc: 0.7585, Val Loss: 0.8016, Val Acc: 0.6859\n",
      "Epoch [75/200], Train Loss: 0.8183, Train Acc: 0.7308, Val Loss: 0.8183, Val Acc: 0.6878\n",
      "Epoch [76/200], Train Loss: 0.8152, Train Acc: 0.7313, Val Loss: 0.8152, Val Acc: 0.6868\n",
      "Epoch [77/200], Train Loss: 0.8061, Train Acc: 0.7484, Val Loss: 0.8061, Val Acc: 0.6917\n",
      "Epoch [78/200], Train Loss: 0.8282, Train Acc: 0.7297, Val Loss: 0.8282, Val Acc: 0.6878\n",
      "Epoch [79/200], Train Loss: 0.8142, Train Acc: 0.7381, Val Loss: 0.8142, Val Acc: 0.6956\n",
      "Epoch [80/200], Train Loss: 0.7960, Train Acc: 0.7663, Val Loss: 0.7960, Val Acc: 0.6966\n",
      "Epoch [81/200], Train Loss: 0.8105, Train Acc: 0.7480, Val Loss: 0.8105, Val Acc: 0.6908\n",
      "Epoch [82/200], Train Loss: 0.7895, Train Acc: 0.7692, Val Loss: 0.7895, Val Acc: 0.6937\n",
      "Epoch [83/200], Train Loss: 0.8084, Train Acc: 0.7528, Val Loss: 0.8084, Val Acc: 0.6956\n",
      "Epoch [84/200], Train Loss: 0.7853, Train Acc: 0.7808, Val Loss: 0.7853, Val Acc: 0.6917\n",
      "Epoch [85/200], Train Loss: 0.7996, Train Acc: 0.7531, Val Loss: 0.7996, Val Acc: 0.6908\n",
      "Epoch [86/200], Train Loss: 0.8071, Train Acc: 0.7543, Val Loss: 0.8071, Val Acc: 0.6917\n",
      "Epoch [87/200], Train Loss: 0.7953, Train Acc: 0.7582, Val Loss: 0.7953, Val Acc: 0.6888\n",
      "Epoch [88/200], Train Loss: 0.8072, Train Acc: 0.7543, Val Loss: 0.8072, Val Acc: 0.6888\n",
      "Epoch [89/200], Train Loss: 0.8040, Train Acc: 0.7512, Val Loss: 0.8040, Val Acc: 0.6868\n",
      "Epoch [90/200], Train Loss: 0.8007, Train Acc: 0.7565, Val Loss: 0.8007, Val Acc: 0.6956\n",
      "Epoch [91/200], Train Loss: 0.8069, Train Acc: 0.7515, Val Loss: 0.8069, Val Acc: 0.6956\n",
      "Epoch [92/200], Train Loss: 0.8002, Train Acc: 0.7549, Val Loss: 0.8002, Val Acc: 0.6878\n",
      "Epoch [93/200], Train Loss: 0.7953, Train Acc: 0.7575, Val Loss: 0.7953, Val Acc: 0.6917\n",
      "Epoch [94/200], Train Loss: 0.8105, Train Acc: 0.7459, Val Loss: 0.8105, Val Acc: 0.6927\n",
      "Epoch [95/200], Train Loss: 0.7891, Train Acc: 0.7765, Val Loss: 0.7891, Val Acc: 0.6908\n",
      "Epoch [96/200], Train Loss: 0.7788, Train Acc: 0.7747, Val Loss: 0.7788, Val Acc: 0.6878\n",
      "Epoch [97/200], Train Loss: 0.7977, Train Acc: 0.7565, Val Loss: 0.7977, Val Acc: 0.6868\n",
      "Epoch [98/200], Train Loss: 0.8100, Train Acc: 0.7426, Val Loss: 0.8100, Val Acc: 0.6917\n",
      "Epoch [99/200], Train Loss: 0.8005, Train Acc: 0.7572, Val Loss: 0.8005, Val Acc: 0.6947\n",
      "Epoch [100/200], Train Loss: 0.8092, Train Acc: 0.7480, Val Loss: 0.8092, Val Acc: 0.6927\n",
      "Epoch [101/200], Train Loss: 0.8049, Train Acc: 0.7514, Val Loss: 0.8049, Val Acc: 0.6937\n",
      "Epoch [102/200], Train Loss: 0.7770, Train Acc: 0.7824, Val Loss: 0.7770, Val Acc: 0.6956\n",
      "Epoch [103/200], Train Loss: 0.7935, Train Acc: 0.7674, Val Loss: 0.7935, Val Acc: 0.6966\n",
      "Epoch [104/200], Train Loss: 0.8134, Train Acc: 0.7431, Val Loss: 0.8134, Val Acc: 0.6888\n",
      "Epoch [105/200], Train Loss: 0.7890, Train Acc: 0.7637, Val Loss: 0.7890, Val Acc: 0.6908\n",
      "Epoch [106/200], Train Loss: 0.7978, Train Acc: 0.7525, Val Loss: 0.7978, Val Acc: 0.6868\n",
      "Epoch [107/200], Train Loss: 0.7924, Train Acc: 0.7622, Val Loss: 0.7924, Val Acc: 0.6859\n",
      "Epoch [108/200], Train Loss: 0.7972, Train Acc: 0.7605, Val Loss: 0.7972, Val Acc: 0.6927\n",
      "Epoch [109/200], Train Loss: 0.7963, Train Acc: 0.7585, Val Loss: 0.7963, Val Acc: 0.6937\n",
      "Epoch [110/200], Train Loss: 0.7897, Train Acc: 0.7613, Val Loss: 0.7897, Val Acc: 0.6898\n",
      "Epoch [111/200], Train Loss: 0.7968, Train Acc: 0.7539, Val Loss: 0.7968, Val Acc: 0.6898\n",
      "Epoch [112/200], Train Loss: 0.7894, Train Acc: 0.7648, Val Loss: 0.7894, Val Acc: 0.6986\n",
      "Epoch [113/200], Train Loss: 0.7937, Train Acc: 0.7634, Val Loss: 0.7937, Val Acc: 0.6908\n",
      "Epoch [114/200], Train Loss: 0.7878, Train Acc: 0.7637, Val Loss: 0.7878, Val Acc: 0.6888\n",
      "Epoch [115/200], Train Loss: 0.7920, Train Acc: 0.7582, Val Loss: 0.7920, Val Acc: 0.6898\n",
      "Epoch [116/200], Train Loss: 0.7882, Train Acc: 0.7677, Val Loss: 0.7882, Val Acc: 0.6966\n",
      "Epoch [117/200], Train Loss: 0.7910, Train Acc: 0.7681, Val Loss: 0.7910, Val Acc: 0.6966\n",
      "Epoch [118/200], Train Loss: 0.8031, Train Acc: 0.7554, Val Loss: 0.8031, Val Acc: 0.6966\n",
      "Epoch [119/200], Train Loss: 0.8129, Train Acc: 0.7370, Val Loss: 0.8129, Val Acc: 0.6927\n",
      "Epoch [120/200], Train Loss: 0.7839, Train Acc: 0.7671, Val Loss: 0.7839, Val Acc: 0.6937\n",
      "Epoch [121/200], Train Loss: 0.8050, Train Acc: 0.7501, Val Loss: 0.8050, Val Acc: 0.6976\n",
      "Epoch [122/200], Train Loss: 0.7969, Train Acc: 0.7606, Val Loss: 0.7969, Val Acc: 0.7005\n",
      "Epoch [123/200], Train Loss: 0.8032, Train Acc: 0.7538, Val Loss: 0.8032, Val Acc: 0.7005\n",
      "Epoch [124/200], Train Loss: 0.8001, Train Acc: 0.7594, Val Loss: 0.8001, Val Acc: 0.6956\n",
      "Epoch [125/200], Train Loss: 0.8053, Train Acc: 0.7525, Val Loss: 0.8053, Val Acc: 0.6956\n",
      "Epoch [126/200], Train Loss: 0.8057, Train Acc: 0.7559, Val Loss: 0.8057, Val Acc: 0.6966\n",
      "Epoch [127/200], Train Loss: 0.7916, Train Acc: 0.7660, Val Loss: 0.7916, Val Acc: 0.6878\n",
      "Epoch [128/200], Train Loss: 0.7821, Train Acc: 0.7685, Val Loss: 0.7821, Val Acc: 0.6898\n",
      "Epoch [129/200], Train Loss: 0.8030, Train Acc: 0.7483, Val Loss: 0.8030, Val Acc: 0.6859\n",
      "Epoch [130/200], Train Loss: 0.7929, Train Acc: 0.7580, Val Loss: 0.7929, Val Acc: 0.6898\n",
      "Epoch [131/200], Train Loss: 0.7914, Train Acc: 0.7695, Val Loss: 0.7914, Val Acc: 0.6947\n",
      "Epoch [132/200], Train Loss: 0.7954, Train Acc: 0.7624, Val Loss: 0.7954, Val Acc: 0.6947\n",
      "Epoch [133/200], Train Loss: 0.7848, Train Acc: 0.7661, Val Loss: 0.7848, Val Acc: 0.6937\n",
      "Epoch [134/200], Train Loss: 0.7879, Train Acc: 0.7682, Val Loss: 0.7879, Val Acc: 0.6966\n",
      "Epoch [135/200], Train Loss: 0.8013, Train Acc: 0.7496, Val Loss: 0.8013, Val Acc: 0.6917\n",
      "Epoch [136/200], Train Loss: 0.7833, Train Acc: 0.7700, Val Loss: 0.7833, Val Acc: 0.6888\n",
      "Epoch [137/200], Train Loss: 0.8027, Train Acc: 0.7501, Val Loss: 0.8027, Val Acc: 0.6927\n",
      "Epoch [138/200], Train Loss: 0.7976, Train Acc: 0.7520, Val Loss: 0.7976, Val Acc: 0.6888\n",
      "Epoch [139/200], Train Loss: 0.7810, Train Acc: 0.7779, Val Loss: 0.7810, Val Acc: 0.6927\n",
      "Epoch [140/200], Train Loss: 0.7934, Train Acc: 0.7570, Val Loss: 0.7934, Val Acc: 0.6937\n",
      "Epoch [141/200], Train Loss: 0.7894, Train Acc: 0.7635, Val Loss: 0.7894, Val Acc: 0.6947\n",
      "Epoch [142/200], Train Loss: 0.7929, Train Acc: 0.7572, Val Loss: 0.7929, Val Acc: 0.6976\n",
      "Epoch [143/200], Train Loss: 0.7923, Train Acc: 0.7632, Val Loss: 0.7923, Val Acc: 0.6966\n",
      "Epoch [144/200], Train Loss: 0.7837, Train Acc: 0.7723, Val Loss: 0.7837, Val Acc: 0.6937\n",
      "Epoch [145/200], Train Loss: 0.7828, Train Acc: 0.7765, Val Loss: 0.7828, Val Acc: 0.6849\n",
      "Epoch [146/200], Train Loss: 0.7865, Train Acc: 0.7669, Val Loss: 0.7865, Val Acc: 0.6878\n",
      "Epoch [147/200], Train Loss: 0.8030, Train Acc: 0.7515, Val Loss: 0.8030, Val Acc: 0.6908\n",
      "Epoch [148/200], Train Loss: 0.7982, Train Acc: 0.7543, Val Loss: 0.7982, Val Acc: 0.6956\n",
      "Epoch [149/200], Train Loss: 0.8021, Train Acc: 0.7499, Val Loss: 0.8021, Val Acc: 0.6986\n",
      "Epoch [150/200], Train Loss: 0.7984, Train Acc: 0.7548, Val Loss: 0.7984, Val Acc: 0.6976\n",
      "Epoch [151/200], Train Loss: 0.7723, Train Acc: 0.7904, Val Loss: 0.7723, Val Acc: 0.6976\n",
      "Epoch [152/200], Train Loss: 0.7932, Train Acc: 0.7626, Val Loss: 0.7932, Val Acc: 0.6917\n",
      "Epoch [153/200], Train Loss: 0.7979, Train Acc: 0.7489, Val Loss: 0.7979, Val Acc: 0.6908\n",
      "Epoch [154/200], Train Loss: 0.7873, Train Acc: 0.7651, Val Loss: 0.7873, Val Acc: 0.6956\n",
      "Epoch [155/200], Train Loss: 0.7944, Train Acc: 0.7605, Val Loss: 0.7944, Val Acc: 0.6947\n",
      "Epoch [156/200], Train Loss: 0.8015, Train Acc: 0.7567, Val Loss: 0.8015, Val Acc: 0.6956\n",
      "Epoch [157/200], Train Loss: 0.7876, Train Acc: 0.7687, Val Loss: 0.7876, Val Acc: 0.6917\n",
      "Epoch [158/200], Train Loss: 0.7791, Train Acc: 0.7765, Val Loss: 0.7791, Val Acc: 0.6898\n",
      "Epoch [159/200], Train Loss: 0.7874, Train Acc: 0.7601, Val Loss: 0.7874, Val Acc: 0.6898\n",
      "Epoch [160/200], Train Loss: 0.8017, Train Acc: 0.7509, Val Loss: 0.8017, Val Acc: 0.6966\n",
      "Epoch [161/200], Train Loss: 0.8039, Train Acc: 0.7493, Val Loss: 0.8039, Val Acc: 0.6976\n",
      "Epoch [162/200], Train Loss: 0.7875, Train Acc: 0.7653, Val Loss: 0.7875, Val Acc: 0.6937\n",
      "Epoch [163/200], Train Loss: 0.7925, Train Acc: 0.7611, Val Loss: 0.7925, Val Acc: 0.6937\n",
      "Epoch [164/200], Train Loss: 0.7922, Train Acc: 0.7589, Val Loss: 0.7922, Val Acc: 0.6898\n",
      "Epoch [165/200], Train Loss: 0.7847, Train Acc: 0.7713, Val Loss: 0.7847, Val Acc: 0.6898\n",
      "Epoch [166/200], Train Loss: 0.8309, Train Acc: 0.7112, Val Loss: 0.8309, Val Acc: 0.6917\n",
      "Epoch [167/200], Train Loss: 0.7899, Train Acc: 0.7631, Val Loss: 0.7899, Val Acc: 0.6888\n",
      "Epoch [168/200], Train Loss: 0.7923, Train Acc: 0.7619, Val Loss: 0.7923, Val Acc: 0.6888\n",
      "Epoch [169/200], Train Loss: 0.8117, Train Acc: 0.7423, Val Loss: 0.8117, Val Acc: 0.6908\n",
      "Epoch [170/200], Train Loss: 0.7994, Train Acc: 0.7530, Val Loss: 0.7994, Val Acc: 0.6956\n",
      "Epoch [171/200], Train Loss: 0.7832, Train Acc: 0.7684, Val Loss: 0.7832, Val Acc: 0.6927\n",
      "Epoch [172/200], Train Loss: 0.8139, Train Acc: 0.7352, Val Loss: 0.8139, Val Acc: 0.6908\n",
      "Epoch [173/200], Train Loss: 0.7886, Train Acc: 0.7635, Val Loss: 0.7886, Val Acc: 0.6927\n",
      "Epoch [174/200], Train Loss: 0.8027, Train Acc: 0.7536, Val Loss: 0.8027, Val Acc: 0.6947\n",
      "Epoch [175/200], Train Loss: 0.7847, Train Acc: 0.7682, Val Loss: 0.7847, Val Acc: 0.6947\n",
      "Epoch [176/200], Train Loss: 0.7849, Train Acc: 0.7716, Val Loss: 0.7849, Val Acc: 0.6908\n",
      "Epoch [177/200], Train Loss: 0.8035, Train Acc: 0.7459, Val Loss: 0.8035, Val Acc: 0.6908\n",
      "Epoch [178/200], Train Loss: 0.7883, Train Acc: 0.7629, Val Loss: 0.7883, Val Acc: 0.6937\n",
      "Epoch [179/200], Train Loss: 0.8002, Train Acc: 0.7536, Val Loss: 0.8002, Val Acc: 0.6966\n",
      "Epoch [180/200], Train Loss: 0.7752, Train Acc: 0.7813, Val Loss: 0.7752, Val Acc: 0.6976\n",
      "Epoch [181/200], Train Loss: 0.7925, Train Acc: 0.7632, Val Loss: 0.7925, Val Acc: 0.6986\n",
      "Epoch [182/200], Train Loss: 0.7967, Train Acc: 0.7590, Val Loss: 0.7967, Val Acc: 0.6966\n",
      "Epoch [183/200], Train Loss: 0.7981, Train Acc: 0.7567, Val Loss: 0.7981, Val Acc: 0.6966\n",
      "Epoch [184/200], Train Loss: 0.7906, Train Acc: 0.7631, Val Loss: 0.7906, Val Acc: 0.6966\n",
      "Epoch [185/200], Train Loss: 0.7877, Train Acc: 0.7694, Val Loss: 0.7877, Val Acc: 0.6976\n",
      "Epoch [186/200], Train Loss: 0.8003, Train Acc: 0.7515, Val Loss: 0.8003, Val Acc: 0.6947\n",
      "Epoch [187/200], Train Loss: 0.7846, Train Acc: 0.7685, Val Loss: 0.7846, Val Acc: 0.6976\n",
      "Epoch [188/200], Train Loss: 0.8023, Train Acc: 0.7525, Val Loss: 0.8023, Val Acc: 0.6976\n",
      "Epoch [189/200], Train Loss: 0.7889, Train Acc: 0.7695, Val Loss: 0.7889, Val Acc: 0.6986\n",
      "Epoch [190/200], Train Loss: 0.7912, Train Acc: 0.7632, Val Loss: 0.7912, Val Acc: 0.6956\n",
      "Epoch [191/200], Train Loss: 0.8090, Train Acc: 0.7452, Val Loss: 0.8090, Val Acc: 0.6888\n",
      "Epoch [192/200], Train Loss: 0.8013, Train Acc: 0.7433, Val Loss: 0.8013, Val Acc: 0.6898\n",
      "Epoch [193/200], Train Loss: 0.7864, Train Acc: 0.7650, Val Loss: 0.7864, Val Acc: 0.6937\n",
      "Epoch [194/200], Train Loss: 0.7828, Train Acc: 0.7700, Val Loss: 0.7828, Val Acc: 0.6947\n",
      "Epoch [195/200], Train Loss: 0.7742, Train Acc: 0.7807, Val Loss: 0.7742, Val Acc: 0.6956\n",
      "Epoch [196/200], Train Loss: 0.7769, Train Acc: 0.7776, Val Loss: 0.7769, Val Acc: 0.6917\n",
      "Epoch [197/200], Train Loss: 0.8056, Train Acc: 0.7446, Val Loss: 0.8056, Val Acc: 0.6956\n",
      "Epoch [198/200], Train Loss: 0.7949, Train Acc: 0.7616, Val Loss: 0.7949, Val Acc: 0.6956\n",
      "Epoch [199/200], Train Loss: 0.8084, Train Acc: 0.7435, Val Loss: 0.8084, Val Acc: 0.6976\n",
      "Epoch [200/200], Train Loss: 0.8040, Train Acc: 0.7477, Val Loss: 0.8040, Val Acc: 0.6966\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    num_epochs=epochs,\n",
    "    train_dataloader=loader_train,\n",
    "    val_dataloader=loader_val,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ed7706ede3cdfcebbe0b86290eed0b0",
     "grade": false,
     "grade_id": "cell-f0963d119d02b778",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's have a look at what we have learned! Create a function that visualizes the decision regions of the network. Overlap it with the points corresponding to the training data and validations data, such as in Section 2, by using the scatter plot function. The training and validation points should have different markers and/or colors. Last, call the function to visualize the decision regions of your network.\n",
    "\n",
    "Hint: A simple way to show the decision region is to generate a lot of points within a predefined range of longitude and latitude and apply your network to it. However, feel free to explore other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa9ff47eb2e3bba211130138b992462",
     "grade": false,
     "grade_id": "cell-38b52ac21b7b7100",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_region(model: nn.Module, x_train, y_train, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Plot the decision region of the model.\n",
    "\n",
    "    Args:\n",
    "    model: nn.Module: The trained model\n",
    "    x_train: The training features, use whatever representation you prefer (e.g. numpy array, torch.Tensor, dataset, dataloader)\n",
    "    y_train: The training labels, use whatever representation you prefer (e.g. numpy array, torch.Tensor, dataset, dataloader)\n",
    "    x_val: The validation features, use whatever representation you prefer (e.g. numpy array, torch.Tensor, dataset, dataloader)\n",
    "    y_val: The validation labels, use whatever representation you prefer (e.g. numpy array, torch.Tensor, dataset, dataloader)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Determine the range for the grid\n",
    "#     x_min, x_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1\n",
    "#     y_min, y_max = x_train[:, 0].min() - 1, x_train[:, 0].max() + 1\n",
    "    \n",
    "#     # Create a grid of points\n",
    "#     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "#                          np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "#     # Flatten the grid to pass through the model\n",
    "#     grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "#     # Use the model to predict on the grid points\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predictions = model(torch.tensor(grid_points, dtype=torch.float32))\n",
    "#         predicted_classes = torch.argmax(predictions, dim=1).numpy()\n",
    "    \n",
    "#     # Reshape the predictions to match the grid shape\n",
    "#     predicted_classes = predicted_classes.reshape(xx.shape)\n",
    "    \n",
    "#     cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "#     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "#     # Plot the decision regions\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.contourf(xx, yy, predicted_classes, alpha=0.3, cmap=cmap_light)\n",
    "    \n",
    "#     # Overlay the training and validation data points\n",
    "#     plt.scatter(x_train[:, 1], x_train[:, 0], c=y_train, cmap=cmap_bold, edgecolor='k', marker='o', label='Train Data')\n",
    "#     plt.scatter(x_val[:, 1], x_val[:, 0], c=y_val, cmap=cmap_bold, edgecolor='k', marker='s', label='Validation Data')\n",
    "    \n",
    "#     # Mapping of classes to names\n",
    "#     pokemon_mapping = {0: 'Diglett', 1: 'Seel', 2: 'Tauros'}\n",
    "    \n",
    "#     # Add custom legend for decision boundaries\n",
    "#     boundary_colors = ['#FFAAAA', '#AAFFAA', '#AAAAFF']\n",
    "#     boundary_patches = [mpatches.Patch(color=boundary_colors[i], label=f'{pokemon_mapping[i]}') for i in range(len(boundary_colors))]\n",
    "    \n",
    "#     # Add labels, legend, and title\n",
    "#     plt.xlabel('Longitude')\n",
    "#     plt.ylabel('Latitude')\n",
    "#     plt.legend(handles=boundary_patches + [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=10, label='Train Data'),\n",
    "#                                            plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='black', markersize=10, label='Validation Data')])\n",
    "#     plt.title('Decision Regions with Training and Validation Data')\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the function\n",
    "# plot_decision_region(model, x_train, y_train, x_val, y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5fa85ea2a29fda505102ade681a780c",
     "grade": false,
     "grade_id": "cell-5bd22b55c4aecfa8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do the learned decision regions look like you would expect? Can they be improved? In what sense, and how would that change your model? Please comment on your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e3a5bdcf2012e5e6a94fa67f3cf2e5e",
     "grade": true,
     "grade_id": "cell-d611e347ddec2871",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d786c397dbdc99f25ccb68813feed32",
     "grade": false,
     "grade_id": "cell-383ffa6dc45f3da3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1. Model capacity and generalization\n",
    "\n",
    "Now we have all the neccessary tools to do a small experiment on model capacity and implications on generalization.\n",
    "\n",
    "Begin by defining a neural network `tiny_model` with a single linear layer and appropriate activation function. Then, train the network until convergence (should be fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23267595ed16c0c4b6d550280a68b430",
     "grade": false,
     "grade_id": "cell-00c94550168dad32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tiny_model(\n",
       "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "class tiny_model(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(tiny_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.softmax(self.fc1(x), dim=1)\n",
    "        return x\n",
    "\n",
    "tiny_model = tiny_model(input_size=2, num_classes=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tiny_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "947da7271bc82b1962e4a2f151e38cc7",
     "grade": true,
     "grade_id": "cell-5177de76d9ba3b95",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(tiny_model, nn.Module), \"tiny_model should be a torch.nn.Module\"\n",
    "test_model_output_shape(tiny_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5210c12d044cbd33369d11ff806d3713",
     "grade": false,
     "grade_id": "cell-08c57975b0623508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, draw the decision regions of the network as you did in the previous section. Before running the code, think about what you expect to see. What will the regions look like? How will they differ from the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3a85fa9cff45032547dc23821abf17a",
     "grade": false,
     "grade_id": "cell-54d26c4853bef495",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/n9y0bc4n5zgb8gg118j_j66h0000gn/T/ipykernel_15008/46787403.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.tensor(prediction == y).float().mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.0622, Train Acc: 0.5082, Val Loss: 1.0622, Val Acc: 0.4137\n",
      "Epoch [2/100], Train Loss: 1.0594, Train Acc: 0.5061, Val Loss: 1.0594, Val Acc: 0.4128\n",
      "Epoch [3/100], Train Loss: 1.0584, Train Acc: 0.5033, Val Loss: 1.0584, Val Acc: 0.4128\n",
      "Epoch [4/100], Train Loss: 1.0568, Train Acc: 0.4991, Val Loss: 1.0568, Val Acc: 0.4128\n",
      "Epoch [5/100], Train Loss: 1.0535, Train Acc: 0.5117, Val Loss: 1.0535, Val Acc: 0.4128\n",
      "Epoch [6/100], Train Loss: 1.0546, Train Acc: 0.5090, Val Loss: 1.0546, Val Acc: 0.4128\n",
      "Epoch [7/100], Train Loss: 1.0547, Train Acc: 0.5020, Val Loss: 1.0547, Val Acc: 0.4118\n",
      "Epoch [8/100], Train Loss: 1.0504, Train Acc: 0.5104, Val Loss: 1.0504, Val Acc: 0.4118\n",
      "Epoch [9/100], Train Loss: 1.0529, Train Acc: 0.5085, Val Loss: 1.0529, Val Acc: 0.4098\n",
      "Epoch [10/100], Train Loss: 1.0533, Train Acc: 0.4975, Val Loss: 1.0533, Val Acc: 0.4079\n",
      "Epoch [11/100], Train Loss: 1.0532, Train Acc: 0.4920, Val Loss: 1.0532, Val Acc: 0.4059\n",
      "Epoch [12/100], Train Loss: 1.0497, Train Acc: 0.4983, Val Loss: 1.0497, Val Acc: 0.4059\n",
      "Epoch [13/100], Train Loss: 1.0538, Train Acc: 0.4899, Val Loss: 1.0538, Val Acc: 0.4059\n",
      "Epoch [14/100], Train Loss: 1.0469, Train Acc: 0.5004, Val Loss: 1.0469, Val Acc: 0.4059\n",
      "Epoch [15/100], Train Loss: 1.0565, Train Acc: 0.4709, Val Loss: 1.0565, Val Acc: 0.4059\n",
      "Epoch [16/100], Train Loss: 1.0513, Train Acc: 0.4835, Val Loss: 1.0513, Val Acc: 0.4059\n",
      "Epoch [17/100], Train Loss: 1.0466, Train Acc: 0.5025, Val Loss: 1.0466, Val Acc: 0.4059\n",
      "Epoch [18/100], Train Loss: 1.0515, Train Acc: 0.4835, Val Loss: 1.0515, Val Acc: 0.4059\n",
      "Epoch [19/100], Train Loss: 1.0450, Train Acc: 0.5046, Val Loss: 1.0450, Val Acc: 0.4059\n",
      "Epoch [20/100], Train Loss: 1.0439, Train Acc: 0.4962, Val Loss: 1.0439, Val Acc: 0.4059\n",
      "Epoch [21/100], Train Loss: 1.0492, Train Acc: 0.4878, Val Loss: 1.0492, Val Acc: 0.4059\n",
      "Epoch [22/100], Train Loss: 1.0462, Train Acc: 0.4905, Val Loss: 1.0462, Val Acc: 0.4059\n",
      "Epoch [23/100], Train Loss: 1.0412, Train Acc: 0.5010, Val Loss: 1.0412, Val Acc: 0.4059\n",
      "Epoch [24/100], Train Loss: 1.0455, Train Acc: 0.4884, Val Loss: 1.0455, Val Acc: 0.4059\n",
      "Epoch [25/100], Train Loss: 1.0408, Train Acc: 0.4968, Val Loss: 1.0408, Val Acc: 0.4059\n",
      "Epoch [26/100], Train Loss: 1.0453, Train Acc: 0.4884, Val Loss: 1.0453, Val Acc: 0.4059\n",
      "Epoch [27/100], Train Loss: 1.0445, Train Acc: 0.4863, Val Loss: 1.0445, Val Acc: 0.4059\n",
      "Epoch [28/100], Train Loss: 1.0448, Train Acc: 0.4842, Val Loss: 1.0448, Val Acc: 0.4059\n",
      "Epoch [29/100], Train Loss: 1.0414, Train Acc: 0.4968, Val Loss: 1.0414, Val Acc: 0.4059\n",
      "Epoch [30/100], Train Loss: 1.0535, Train Acc: 0.4505, Val Loss: 1.0535, Val Acc: 0.4059\n",
      "Epoch [31/100], Train Loss: 1.0323, Train Acc: 0.5137, Val Loss: 1.0323, Val Acc: 0.4059\n",
      "Epoch [32/100], Train Loss: 1.0356, Train Acc: 0.5094, Val Loss: 1.0356, Val Acc: 0.4059\n",
      "Epoch [33/100], Train Loss: 1.0341, Train Acc: 0.5031, Val Loss: 1.0341, Val Acc: 0.4059\n",
      "Epoch [34/100], Train Loss: 1.0364, Train Acc: 0.4926, Val Loss: 1.0364, Val Acc: 0.4059\n",
      "Epoch [35/100], Train Loss: 1.0388, Train Acc: 0.4884, Val Loss: 1.0388, Val Acc: 0.4059\n",
      "Epoch [36/100], Train Loss: 1.0351, Train Acc: 0.4884, Val Loss: 1.0351, Val Acc: 0.4059\n",
      "Epoch [37/100], Train Loss: 1.0306, Train Acc: 0.5073, Val Loss: 1.0306, Val Acc: 0.4059\n",
      "Epoch [38/100], Train Loss: 1.0376, Train Acc: 0.4947, Val Loss: 1.0376, Val Acc: 0.4059\n",
      "Epoch [39/100], Train Loss: 1.0329, Train Acc: 0.4989, Val Loss: 1.0329, Val Acc: 0.4059\n",
      "Epoch [40/100], Train Loss: 1.0367, Train Acc: 0.4863, Val Loss: 1.0367, Val Acc: 0.4059\n",
      "Epoch [41/100], Train Loss: 1.0300, Train Acc: 0.4947, Val Loss: 1.0300, Val Acc: 0.4059\n",
      "Epoch [42/100], Train Loss: 1.0364, Train Acc: 0.4926, Val Loss: 1.0364, Val Acc: 0.4059\n",
      "Epoch [43/100], Train Loss: 1.0267, Train Acc: 0.5137, Val Loss: 1.0267, Val Acc: 0.4059\n",
      "Epoch [44/100], Train Loss: 1.0311, Train Acc: 0.4947, Val Loss: 1.0311, Val Acc: 0.4059\n",
      "Epoch [45/100], Train Loss: 1.0232, Train Acc: 0.5094, Val Loss: 1.0232, Val Acc: 0.4059\n",
      "Epoch [46/100], Train Loss: 1.0243, Train Acc: 0.5094, Val Loss: 1.0243, Val Acc: 0.4059\n",
      "Epoch [47/100], Train Loss: 1.0287, Train Acc: 0.5010, Val Loss: 1.0287, Val Acc: 0.4059\n",
      "Epoch [48/100], Train Loss: 1.0308, Train Acc: 0.4884, Val Loss: 1.0308, Val Acc: 0.4059\n",
      "Epoch [49/100], Train Loss: 1.0231, Train Acc: 0.5115, Val Loss: 1.0231, Val Acc: 0.4059\n",
      "Epoch [50/100], Train Loss: 1.0348, Train Acc: 0.4758, Val Loss: 1.0348, Val Acc: 0.4059\n",
      "Epoch [51/100], Train Loss: 1.0221, Train Acc: 0.5115, Val Loss: 1.0221, Val Acc: 0.4059\n",
      "Epoch [52/100], Train Loss: 1.0218, Train Acc: 0.5031, Val Loss: 1.0218, Val Acc: 0.4059\n",
      "Epoch [53/100], Train Loss: 1.0243, Train Acc: 0.5052, Val Loss: 1.0243, Val Acc: 0.4059\n",
      "Epoch [54/100], Train Loss: 1.0230, Train Acc: 0.5094, Val Loss: 1.0230, Val Acc: 0.4059\n",
      "Epoch [55/100], Train Loss: 1.0283, Train Acc: 0.4905, Val Loss: 1.0283, Val Acc: 0.4059\n",
      "Epoch [56/100], Train Loss: 1.0222, Train Acc: 0.4989, Val Loss: 1.0222, Val Acc: 0.4059\n",
      "Epoch [57/100], Train Loss: 1.0243, Train Acc: 0.4905, Val Loss: 1.0243, Val Acc: 0.4059\n",
      "Epoch [58/100], Train Loss: 1.0340, Train Acc: 0.4779, Val Loss: 1.0340, Val Acc: 0.4059\n",
      "Epoch [59/100], Train Loss: 1.0177, Train Acc: 0.5137, Val Loss: 1.0177, Val Acc: 0.4059\n",
      "Epoch [60/100], Train Loss: 1.0255, Train Acc: 0.4884, Val Loss: 1.0255, Val Acc: 0.4069\n",
      "Epoch [61/100], Train Loss: 1.0284, Train Acc: 0.4891, Val Loss: 1.0284, Val Acc: 0.4069\n",
      "Epoch [62/100], Train Loss: 1.0148, Train Acc: 0.5038, Val Loss: 1.0148, Val Acc: 0.4069\n",
      "Epoch [63/100], Train Loss: 1.0229, Train Acc: 0.4989, Val Loss: 1.0229, Val Acc: 0.4069\n",
      "Epoch [64/100], Train Loss: 1.0317, Train Acc: 0.4743, Val Loss: 1.0317, Val Acc: 0.4069\n",
      "Epoch [65/100], Train Loss: 1.0246, Train Acc: 0.4954, Val Loss: 1.0246, Val Acc: 0.4069\n",
      "Epoch [66/100], Train Loss: 1.0168, Train Acc: 0.5017, Val Loss: 1.0168, Val Acc: 0.4069\n",
      "Epoch [67/100], Train Loss: 1.0179, Train Acc: 0.5059, Val Loss: 1.0179, Val Acc: 0.4069\n",
      "Epoch [68/100], Train Loss: 1.0197, Train Acc: 0.4870, Val Loss: 1.0197, Val Acc: 0.4069\n",
      "Epoch [69/100], Train Loss: 1.0191, Train Acc: 0.4912, Val Loss: 1.0191, Val Acc: 0.4069\n",
      "Epoch [70/100], Train Loss: 1.0125, Train Acc: 0.5059, Val Loss: 1.0125, Val Acc: 0.4069\n",
      "Epoch [71/100], Train Loss: 1.0186, Train Acc: 0.5017, Val Loss: 1.0186, Val Acc: 0.4069\n",
      "Epoch [72/100], Train Loss: 1.0213, Train Acc: 0.4870, Val Loss: 1.0213, Val Acc: 0.4069\n",
      "Epoch [73/100], Train Loss: 1.0248, Train Acc: 0.4764, Val Loss: 1.0248, Val Acc: 0.4069\n",
      "Epoch [74/100], Train Loss: 1.0149, Train Acc: 0.5059, Val Loss: 1.0149, Val Acc: 0.4069\n",
      "Epoch [75/100], Train Loss: 1.0196, Train Acc: 0.4806, Val Loss: 1.0196, Val Acc: 0.4069\n",
      "Epoch [76/100], Train Loss: 1.0176, Train Acc: 0.4933, Val Loss: 1.0176, Val Acc: 0.4069\n",
      "Epoch [77/100], Train Loss: 1.0146, Train Acc: 0.5038, Val Loss: 1.0146, Val Acc: 0.4069\n",
      "Epoch [78/100], Train Loss: 1.0273, Train Acc: 0.4617, Val Loss: 1.0273, Val Acc: 0.4069\n",
      "Epoch [79/100], Train Loss: 1.0084, Train Acc: 0.5059, Val Loss: 1.0084, Val Acc: 0.4069\n",
      "Epoch [80/100], Train Loss: 1.0084, Train Acc: 0.5185, Val Loss: 1.0084, Val Acc: 0.4069\n",
      "Epoch [81/100], Train Loss: 1.0150, Train Acc: 0.4912, Val Loss: 1.0150, Val Acc: 0.4069\n",
      "Epoch [82/100], Train Loss: 1.0168, Train Acc: 0.4848, Val Loss: 1.0168, Val Acc: 0.4069\n",
      "Epoch [83/100], Train Loss: 1.0155, Train Acc: 0.4912, Val Loss: 1.0155, Val Acc: 0.4069\n",
      "Epoch [84/100], Train Loss: 1.0181, Train Acc: 0.4806, Val Loss: 1.0181, Val Acc: 0.4069\n",
      "Epoch [85/100], Train Loss: 1.0146, Train Acc: 0.4870, Val Loss: 1.0146, Val Acc: 0.4069\n",
      "Epoch [86/100], Train Loss: 1.0131, Train Acc: 0.4933, Val Loss: 1.0131, Val Acc: 0.4069\n",
      "Epoch [87/100], Train Loss: 1.0120, Train Acc: 0.4884, Val Loss: 1.0120, Val Acc: 0.4069\n",
      "Epoch [88/100], Train Loss: 1.0144, Train Acc: 0.4905, Val Loss: 1.0144, Val Acc: 0.4069\n",
      "Epoch [89/100], Train Loss: 1.0072, Train Acc: 0.4926, Val Loss: 1.0072, Val Acc: 0.4069\n",
      "Epoch [90/100], Train Loss: 1.0191, Train Acc: 0.4891, Val Loss: 1.0191, Val Acc: 0.4069\n",
      "Epoch [91/100], Train Loss: 1.0219, Train Acc: 0.4680, Val Loss: 1.0219, Val Acc: 0.4069\n",
      "Epoch [92/100], Train Loss: 1.0124, Train Acc: 0.5017, Val Loss: 1.0124, Val Acc: 0.4069\n",
      "Epoch [93/100], Train Loss: 1.0147, Train Acc: 0.4870, Val Loss: 1.0147, Val Acc: 0.4069\n",
      "Epoch [94/100], Train Loss: 1.0050, Train Acc: 0.5080, Val Loss: 1.0050, Val Acc: 0.4069\n",
      "Epoch [95/100], Train Loss: 1.0074, Train Acc: 0.5080, Val Loss: 1.0074, Val Acc: 0.4069\n",
      "Epoch [96/100], Train Loss: 1.0089, Train Acc: 0.4996, Val Loss: 1.0089, Val Acc: 0.4069\n",
      "Epoch [97/100], Train Loss: 1.0127, Train Acc: 0.4912, Val Loss: 1.0127, Val Acc: 0.4069\n",
      "Epoch [98/100], Train Loss: 1.0124, Train Acc: 0.4827, Val Loss: 1.0124, Val Acc: 0.4069\n",
      "Epoch [99/100], Train Loss: 1.0027, Train Acc: 0.5017, Val Loss: 1.0027, Val Acc: 0.4069\n",
      "Epoch [100/100], Train Loss: 1.0161, Train Acc: 0.4827, Val Loss: 1.0161, Val Acc: 0.4069\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define the loss function and optimizer\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(tiny_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=tiny_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    num_epochs=epochs,\n",
    "    train_dataloader=loader_train,\n",
    "    val_dataloader=loader_val,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# plot_decision_region(tiny_model, x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7dd25b5a822f76d673abd2c19d04214",
     "grade": false,
     "grade_id": "cell-60ac824096c93c40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Are the decision regions any different? Why/why not? What does this tell you about the model capacity and generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fab04c63006230a270e8d4cb6de097c",
     "grade": true,
     "grade_id": "cell-366b5ee2a98cf990",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e3581287819f1a7727aeed0aae5e3e8",
     "grade": false,
     "grade_id": "cell-9b64a4381757316b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, create a neural network `large_model` with many layers and/or hidden units. Try to maximize the training accuracy (0.7-0.8). Getting these models to converge might be a bit tricky, so you might have to adjust the learning rate, the optimizer, etc. Also, might be easier to have a wide model (large number of hidden units, say 1024 per layer) than a deep model (more than 4 layers).\n",
    "\n",
    "Feel free to experiment, but if you get stuck, our model is a MLP with an input layer, 4 hidden layers with 1024 units each, and an output layer. We used the ReLU activation function between each layer and a softmax for the output layer. We used the Adam optimizer with a learning rate of 0.003 and a batch size of 512. We trained for at least a few hundred epochs. Also, normalizing the input can be helpful (zero mean and unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f70d4de2747cd6ef9d0d501495dcb1a",
     "grade": false,
     "grade_id": "cell-9abb82db6e8413d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "large_model(\n",
       "  (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc5): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "class large_model(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(large_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512) \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.softmax(self.fc5(x), dim=1)\n",
    "        return x \n",
    "    \n",
    "large_model = large_model(input_size=2, num_classes=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "large_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "380a2b65c2bb0730aa6cb147e7c4e74f",
     "grade": true,
     "grade_id": "cell-098f92715968886f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(large_model, nn.Module), \"large_model should be a torch.nn.Module\"\n",
    "test_model_output_shape(large_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f52e36900e80e083dbc6ba73f0d000d5",
     "grade": false,
     "grade_id": "cell-ea77af681fd56aaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Again, draw the decision regions of the network. What do you expect to see now? How will the regions differ from the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88522dc56161b5b0ec1e47709327f81e",
     "grade": false,
     "grade_id": "cell-2386af80723e8770",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/n9y0bc4n5zgb8gg118j_j66h0000gn/T/ipykernel_15008/46787403.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.tensor(prediction == y).float().mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.0572, Train Acc: 0.4504, Val Loss: 1.0572, Val Acc: 0.4810\n",
      "Epoch [2/200], Train Loss: 0.9336, Train Acc: 0.5873, Val Loss: 0.9336, Val Acc: 0.5825\n",
      "Epoch [3/200], Train Loss: 0.8910, Train Acc: 0.6603, Val Loss: 0.8910, Val Acc: 0.5932\n",
      "Epoch [4/200], Train Loss: 0.8558, Train Acc: 0.6752, Val Loss: 0.8558, Val Acc: 0.6546\n",
      "Epoch [5/200], Train Loss: 0.8693, Train Acc: 0.6740, Val Loss: 0.8693, Val Acc: 0.5893\n",
      "Epoch [6/200], Train Loss: 0.8778, Train Acc: 0.6672, Val Loss: 0.8778, Val Acc: 0.5961\n",
      "Epoch [7/200], Train Loss: 0.8501, Train Acc: 0.6967, Val Loss: 0.8501, Val Acc: 0.5971\n",
      "Epoch [8/200], Train Loss: 0.8766, Train Acc: 0.6658, Val Loss: 0.8766, Val Acc: 0.6868\n",
      "Epoch [9/200], Train Loss: 0.8360, Train Acc: 0.7096, Val Loss: 0.8360, Val Acc: 0.6868\n",
      "Epoch [10/200], Train Loss: 0.8153, Train Acc: 0.7370, Val Loss: 0.8153, Val Acc: 0.6245\n",
      "Epoch [11/200], Train Loss: 0.8130, Train Acc: 0.7395, Val Loss: 0.8130, Val Acc: 0.6790\n",
      "Epoch [12/200], Train Loss: 0.8266, Train Acc: 0.7236, Val Loss: 0.8266, Val Acc: 0.6859\n",
      "Epoch [13/200], Train Loss: 0.8279, Train Acc: 0.7247, Val Loss: 0.8279, Val Acc: 0.6868\n",
      "Epoch [14/200], Train Loss: 0.8028, Train Acc: 0.7479, Val Loss: 0.8028, Val Acc: 0.6878\n",
      "Epoch [15/200], Train Loss: 0.8089, Train Acc: 0.7403, Val Loss: 0.8089, Val Acc: 0.6868\n",
      "Epoch [16/200], Train Loss: 0.8180, Train Acc: 0.7332, Val Loss: 0.8180, Val Acc: 0.6868\n",
      "Epoch [17/200], Train Loss: 0.8176, Train Acc: 0.7332, Val Loss: 0.8176, Val Acc: 0.6859\n",
      "Epoch [18/200], Train Loss: 0.8051, Train Acc: 0.7458, Val Loss: 0.8051, Val Acc: 0.6284\n",
      "Epoch [19/200], Train Loss: 0.8045, Train Acc: 0.7464, Val Loss: 0.8045, Val Acc: 0.6303\n",
      "Epoch [20/200], Train Loss: 0.8052, Train Acc: 0.7458, Val Loss: 0.8052, Val Acc: 0.6868\n",
      "Epoch [21/200], Train Loss: 0.8188, Train Acc: 0.7304, Val Loss: 0.8188, Val Acc: 0.6878\n",
      "Epoch [22/200], Train Loss: 0.8301, Train Acc: 0.7239, Val Loss: 0.8301, Val Acc: 0.6859\n",
      "Epoch [23/200], Train Loss: 0.8197, Train Acc: 0.7317, Val Loss: 0.8197, Val Acc: 0.6868\n",
      "Epoch [24/200], Train Loss: 0.8163, Train Acc: 0.7353, Val Loss: 0.8163, Val Acc: 0.6868\n",
      "Epoch [25/200], Train Loss: 0.8059, Train Acc: 0.7458, Val Loss: 0.8059, Val Acc: 0.6868\n",
      "Epoch [26/200], Train Loss: 0.8143, Train Acc: 0.7374, Val Loss: 0.8143, Val Acc: 0.6868\n",
      "Epoch [27/200], Train Loss: 0.8084, Train Acc: 0.7437, Val Loss: 0.8084, Val Acc: 0.6868\n",
      "Epoch [28/200], Train Loss: 0.8143, Train Acc: 0.7374, Val Loss: 0.8143, Val Acc: 0.6868\n",
      "Epoch [29/200], Train Loss: 0.8183, Train Acc: 0.7332, Val Loss: 0.8183, Val Acc: 0.6859\n",
      "Epoch [30/200], Train Loss: 0.8230, Train Acc: 0.7283, Val Loss: 0.8230, Val Acc: 0.6839\n",
      "Epoch [31/200], Train Loss: 0.8133, Train Acc: 0.7375, Val Loss: 0.8133, Val Acc: 0.6868\n",
      "Epoch [32/200], Train Loss: 0.8040, Train Acc: 0.7506, Val Loss: 0.8040, Val Acc: 0.6849\n",
      "Epoch [33/200], Train Loss: 0.8151, Train Acc: 0.7362, Val Loss: 0.8151, Val Acc: 0.6800\n",
      "Epoch [34/200], Train Loss: 0.8515, Train Acc: 0.7003, Val Loss: 0.8515, Val Acc: 0.6215\n",
      "Epoch [35/200], Train Loss: 0.8293, Train Acc: 0.7178, Val Loss: 0.8293, Val Acc: 0.6585\n",
      "Epoch [36/200], Train Loss: 0.8465, Train Acc: 0.7046, Val Loss: 0.8465, Val Acc: 0.6868\n",
      "Epoch [37/200], Train Loss: 0.8342, Train Acc: 0.7182, Val Loss: 0.8342, Val Acc: 0.6849\n",
      "Epoch [38/200], Train Loss: 0.8330, Train Acc: 0.7175, Val Loss: 0.8330, Val Acc: 0.6859\n",
      "Epoch [39/200], Train Loss: 0.8255, Train Acc: 0.7263, Val Loss: 0.8255, Val Acc: 0.6274\n",
      "Epoch [40/200], Train Loss: 0.8179, Train Acc: 0.7338, Val Loss: 0.8179, Val Acc: 0.6868\n",
      "Epoch [41/200], Train Loss: 0.8247, Train Acc: 0.7262, Val Loss: 0.8247, Val Acc: 0.6868\n",
      "Epoch [42/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6849\n",
      "Epoch [43/200], Train Loss: 0.8128, Train Acc: 0.7395, Val Loss: 0.8128, Val Acc: 0.6186\n",
      "Epoch [44/200], Train Loss: 0.8263, Train Acc: 0.7226, Val Loss: 0.8263, Val Acc: 0.6888\n",
      "Epoch [45/200], Train Loss: 0.8281, Train Acc: 0.7224, Val Loss: 0.8281, Val Acc: 0.6868\n",
      "Epoch [46/200], Train Loss: 0.8108, Train Acc: 0.7408, Val Loss: 0.8108, Val Acc: 0.6108\n",
      "Epoch [47/200], Train Loss: 0.8338, Train Acc: 0.7161, Val Loss: 0.8338, Val Acc: 0.6868\n",
      "Epoch [48/200], Train Loss: 0.8085, Train Acc: 0.7445, Val Loss: 0.8085, Val Acc: 0.6878\n",
      "Epoch [49/200], Train Loss: 0.8190, Train Acc: 0.7325, Val Loss: 0.8190, Val Acc: 0.6878\n",
      "Epoch [50/200], Train Loss: 0.8153, Train Acc: 0.7346, Val Loss: 0.8153, Val Acc: 0.6868\n",
      "Epoch [51/200], Train Loss: 0.8282, Train Acc: 0.7233, Val Loss: 0.8282, Val Acc: 0.6888\n",
      "Epoch [52/200], Train Loss: 0.8199, Train Acc: 0.7317, Val Loss: 0.8199, Val Acc: 0.6878\n",
      "Epoch [53/200], Train Loss: 0.8245, Train Acc: 0.7268, Val Loss: 0.8245, Val Acc: 0.6878\n",
      "Epoch [54/200], Train Loss: 0.8099, Train Acc: 0.7416, Val Loss: 0.8099, Val Acc: 0.6888\n",
      "Epoch [55/200], Train Loss: 0.8152, Train Acc: 0.7366, Val Loss: 0.8152, Val Acc: 0.6888\n",
      "Epoch [56/200], Train Loss: 0.8025, Train Acc: 0.7492, Val Loss: 0.8025, Val Acc: 0.6878\n",
      "Epoch [57/200], Train Loss: 0.8133, Train Acc: 0.7380, Val Loss: 0.8133, Val Acc: 0.6888\n",
      "Epoch [58/200], Train Loss: 0.8175, Train Acc: 0.7338, Val Loss: 0.8175, Val Acc: 0.6878\n",
      "Epoch [59/200], Train Loss: 0.8099, Train Acc: 0.7416, Val Loss: 0.8099, Val Acc: 0.6878\n",
      "Epoch [60/200], Train Loss: 0.8226, Train Acc: 0.7290, Val Loss: 0.8226, Val Acc: 0.6878\n",
      "Epoch [61/200], Train Loss: 0.8225, Train Acc: 0.7290, Val Loss: 0.8225, Val Acc: 0.6878\n",
      "Epoch [62/200], Train Loss: 0.8120, Train Acc: 0.7395, Val Loss: 0.8120, Val Acc: 0.6878\n",
      "Epoch [63/200], Train Loss: 0.8201, Train Acc: 0.7311, Val Loss: 0.8201, Val Acc: 0.6878\n",
      "Epoch [64/200], Train Loss: 0.8198, Train Acc: 0.7324, Val Loss: 0.8198, Val Acc: 0.6878\n",
      "Epoch [65/200], Train Loss: 0.8182, Train Acc: 0.7332, Val Loss: 0.8182, Val Acc: 0.6878\n",
      "Epoch [66/200], Train Loss: 0.8162, Train Acc: 0.7353, Val Loss: 0.8162, Val Acc: 0.6878\n",
      "Epoch [67/200], Train Loss: 0.8372, Train Acc: 0.7142, Val Loss: 0.8372, Val Acc: 0.6878\n",
      "Epoch [68/200], Train Loss: 0.8315, Train Acc: 0.7205, Val Loss: 0.8315, Val Acc: 0.6878\n",
      "Epoch [69/200], Train Loss: 0.8288, Train Acc: 0.7226, Val Loss: 0.8288, Val Acc: 0.6849\n",
      "Epoch [70/200], Train Loss: 0.8047, Train Acc: 0.7464, Val Loss: 0.8047, Val Acc: 0.6284\n",
      "Epoch [71/200], Train Loss: 0.8061, Train Acc: 0.7451, Val Loss: 0.8061, Val Acc: 0.6878\n",
      "Epoch [72/200], Train Loss: 0.8058, Train Acc: 0.7458, Val Loss: 0.8058, Val Acc: 0.6878\n",
      "Epoch [73/200], Train Loss: 0.8135, Train Acc: 0.7388, Val Loss: 0.8135, Val Acc: 0.6878\n",
      "Epoch [74/200], Train Loss: 0.8162, Train Acc: 0.7346, Val Loss: 0.8162, Val Acc: 0.6868\n",
      "Epoch [75/200], Train Loss: 0.8161, Train Acc: 0.7353, Val Loss: 0.8161, Val Acc: 0.6186\n",
      "Epoch [76/200], Train Loss: 0.8242, Train Acc: 0.7260, Val Loss: 0.8242, Val Acc: 0.6264\n",
      "Epoch [77/200], Train Loss: 0.8167, Train Acc: 0.7346, Val Loss: 0.8167, Val Acc: 0.6878\n",
      "Epoch [78/200], Train Loss: 0.8086, Train Acc: 0.7430, Val Loss: 0.8086, Val Acc: 0.6868\n",
      "Epoch [79/200], Train Loss: 0.8094, Train Acc: 0.7411, Val Loss: 0.8094, Val Acc: 0.6878\n",
      "Epoch [80/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [81/200], Train Loss: 0.8169, Train Acc: 0.7353, Val Loss: 0.8169, Val Acc: 0.6878\n",
      "Epoch [82/200], Train Loss: 0.8275, Train Acc: 0.7246, Val Loss: 0.8275, Val Acc: 0.6215\n",
      "Epoch [83/200], Train Loss: 0.8333, Train Acc: 0.7168, Val Loss: 0.8333, Val Acc: 0.6878\n",
      "Epoch [84/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [85/200], Train Loss: 0.8231, Train Acc: 0.7283, Val Loss: 0.8231, Val Acc: 0.6878\n",
      "Epoch [86/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [87/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [88/200], Train Loss: 0.8337, Train Acc: 0.7178, Val Loss: 0.8337, Val Acc: 0.6878\n",
      "Epoch [89/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [90/200], Train Loss: 0.8315, Train Acc: 0.7199, Val Loss: 0.8315, Val Acc: 0.6878\n",
      "Epoch [91/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [92/200], Train Loss: 0.7936, Train Acc: 0.7578, Val Loss: 0.7936, Val Acc: 0.6878\n",
      "Epoch [93/200], Train Loss: 0.8189, Train Acc: 0.7325, Val Loss: 0.8189, Val Acc: 0.6878\n",
      "Epoch [94/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [95/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [96/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [97/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6878\n",
      "Epoch [98/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [99/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [100/200], Train Loss: 0.8021, Train Acc: 0.7493, Val Loss: 0.8021, Val Acc: 0.6878\n",
      "Epoch [101/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [102/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [103/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [104/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [105/200], Train Loss: 0.8021, Train Acc: 0.7493, Val Loss: 0.8021, Val Acc: 0.6878\n",
      "Epoch [106/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [107/200], Train Loss: 0.8021, Train Acc: 0.7493, Val Loss: 0.8021, Val Acc: 0.6878\n",
      "Epoch [108/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [109/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [110/200], Train Loss: 0.8337, Train Acc: 0.7178, Val Loss: 0.8337, Val Acc: 0.6878\n",
      "Epoch [111/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [112/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [113/200], Train Loss: 0.8021, Train Acc: 0.7493, Val Loss: 0.8021, Val Acc: 0.6878\n",
      "Epoch [114/200], Train Loss: 0.8042, Train Acc: 0.7472, Val Loss: 0.8042, Val Acc: 0.6878\n",
      "Epoch [115/200], Train Loss: 0.8379, Train Acc: 0.7136, Val Loss: 0.8379, Val Acc: 0.6878\n",
      "Epoch [116/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6878\n",
      "Epoch [117/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [118/200], Train Loss: 0.8189, Train Acc: 0.7325, Val Loss: 0.8189, Val Acc: 0.6878\n",
      "Epoch [119/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [120/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [121/200], Train Loss: 0.8274, Train Acc: 0.7241, Val Loss: 0.8274, Val Acc: 0.6878\n",
      "Epoch [122/200], Train Loss: 0.8274, Train Acc: 0.7241, Val Loss: 0.8274, Val Acc: 0.6878\n",
      "Epoch [123/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [124/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [125/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [126/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [127/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [128/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [129/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [130/200], Train Loss: 0.8231, Train Acc: 0.7283, Val Loss: 0.8231, Val Acc: 0.6878\n",
      "Epoch [131/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [132/200], Train Loss: 0.8189, Train Acc: 0.7325, Val Loss: 0.8189, Val Acc: 0.6878\n",
      "Epoch [133/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [134/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [135/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [136/200], Train Loss: 0.8274, Train Acc: 0.7241, Val Loss: 0.8274, Val Acc: 0.6878\n",
      "Epoch [137/200], Train Loss: 0.8231, Train Acc: 0.7283, Val Loss: 0.8231, Val Acc: 0.6878\n",
      "Epoch [138/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [139/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [140/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [141/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6878\n",
      "Epoch [142/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [143/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [144/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [145/200], Train Loss: 0.7937, Train Acc: 0.7578, Val Loss: 0.7937, Val Acc: 0.6878\n",
      "Epoch [146/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [147/200], Train Loss: 0.8295, Train Acc: 0.7220, Val Loss: 0.8295, Val Acc: 0.6878\n",
      "Epoch [148/200], Train Loss: 0.8295, Train Acc: 0.7220, Val Loss: 0.8295, Val Acc: 0.6878\n",
      "Epoch [149/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [150/200], Train Loss: 0.8000, Train Acc: 0.7514, Val Loss: 0.8000, Val Acc: 0.6878\n",
      "Epoch [151/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [152/200], Train Loss: 0.7895, Train Acc: 0.7620, Val Loss: 0.7895, Val Acc: 0.6878\n",
      "Epoch [153/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6878\n",
      "Epoch [154/200], Train Loss: 0.8274, Train Acc: 0.7241, Val Loss: 0.8274, Val Acc: 0.6878\n",
      "Epoch [155/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [156/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [157/200], Train Loss: 0.8337, Train Acc: 0.7178, Val Loss: 0.8337, Val Acc: 0.6878\n",
      "Epoch [158/200], Train Loss: 0.8189, Train Acc: 0.7325, Val Loss: 0.8189, Val Acc: 0.6878\n",
      "Epoch [159/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [160/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [161/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [162/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [163/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [164/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6878\n",
      "Epoch [165/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [166/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [167/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [168/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [169/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [170/200], Train Loss: 0.8084, Train Acc: 0.7430, Val Loss: 0.8084, Val Acc: 0.6878\n",
      "Epoch [171/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [172/200], Train Loss: 0.8042, Train Acc: 0.7472, Val Loss: 0.8042, Val Acc: 0.6878\n",
      "Epoch [173/200], Train Loss: 0.7916, Train Acc: 0.7599, Val Loss: 0.7916, Val Acc: 0.6878\n",
      "Epoch [174/200], Train Loss: 0.8337, Train Acc: 0.7178, Val Loss: 0.8337, Val Acc: 0.6878\n",
      "Epoch [175/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [176/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [177/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [178/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [179/200], Train Loss: 0.8400, Train Acc: 0.7115, Val Loss: 0.8400, Val Acc: 0.6878\n",
      "Epoch [180/200], Train Loss: 0.7979, Train Acc: 0.7535, Val Loss: 0.7979, Val Acc: 0.6878\n",
      "Epoch [181/200], Train Loss: 0.8042, Train Acc: 0.7472, Val Loss: 0.8042, Val Acc: 0.6878\n",
      "Epoch [182/200], Train Loss: 0.7937, Train Acc: 0.7578, Val Loss: 0.7937, Val Acc: 0.6878\n",
      "Epoch [183/200], Train Loss: 0.8274, Train Acc: 0.7241, Val Loss: 0.8274, Val Acc: 0.6878\n",
      "Epoch [184/200], Train Loss: 0.8063, Train Acc: 0.7451, Val Loss: 0.8063, Val Acc: 0.6878\n",
      "Epoch [185/200], Train Loss: 0.8147, Train Acc: 0.7367, Val Loss: 0.8147, Val Acc: 0.6878\n",
      "Epoch [186/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [187/200], Train Loss: 0.8210, Train Acc: 0.7304, Val Loss: 0.8210, Val Acc: 0.6878\n",
      "Epoch [188/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [189/200], Train Loss: 0.8148, Train Acc: 0.7361, Val Loss: 0.8148, Val Acc: 0.6878\n",
      "Epoch [190/200], Train Loss: 0.8105, Train Acc: 0.7409, Val Loss: 0.8105, Val Acc: 0.6878\n",
      "Epoch [191/200], Train Loss: 0.8126, Train Acc: 0.7388, Val Loss: 0.8126, Val Acc: 0.6878\n",
      "Epoch [192/200], Train Loss: 0.8252, Train Acc: 0.7262, Val Loss: 0.8252, Val Acc: 0.6878\n",
      "Epoch [193/200], Train Loss: 0.8044, Train Acc: 0.7472, Val Loss: 0.8044, Val Acc: 0.6878\n",
      "Epoch [194/200], Train Loss: 0.8168, Train Acc: 0.7346, Val Loss: 0.8168, Val Acc: 0.6859\n",
      "Epoch [195/200], Train Loss: 0.8069, Train Acc: 0.7445, Val Loss: 0.8069, Val Acc: 0.6859\n",
      "Epoch [196/200], Train Loss: 0.8077, Train Acc: 0.7438, Val Loss: 0.8077, Val Acc: 0.6859\n",
      "Epoch [197/200], Train Loss: 0.8432, Train Acc: 0.7081, Val Loss: 0.8432, Val Acc: 0.6888\n",
      "Epoch [198/200], Train Loss: 0.8024, Train Acc: 0.7487, Val Loss: 0.8024, Val Acc: 0.6878\n",
      "Epoch [199/200], Train Loss: 0.8223, Train Acc: 0.7291, Val Loss: 0.8223, Val Acc: 0.6868\n",
      "Epoch [200/200], Train Loss: 0.7936, Train Acc: 0.7573, Val Loss: 0.7936, Val Acc: 0.6868\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "epochs = 200\n",
    "learning_rate = 0.002\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(large_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=large_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    num_epochs=epochs,\n",
    "    train_dataloader=loader_train,\n",
    "    val_dataloader=loader_val,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# def plot_results(train_losses, val_losses, train_accs, val_accs):\n",
    "#   epochs = range(1, len(train_losses) + 1)\n",
    "  \n",
    "#   plt.figure(figsize=(17, 6))\n",
    "  \n",
    "#   # Plotting Loss\n",
    "#   plt.subplot(1, 2, 1)\n",
    "#   plt.plot(epochs, train_losses, label='Training Loss')\n",
    "#   plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "#   plt.xlabel('Epochs')\n",
    "#   plt.ylabel('Loss')\n",
    "#   plt.title('Training and Validation Loss')\n",
    "#   plt.legend()\n",
    "  \n",
    "#   # Plotting Accuracy\n",
    "#   plt.subplot(1, 2, 2)\n",
    "#   plt.plot(epochs, train_accs, label='Training Accuracy')\n",
    "#   plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
    "#   plt.xlabel('Epochs')\n",
    "#   plt.ylabel('Accuracy')\n",
    "#   plt.title('Training and Validation Accuracy')\n",
    "#   plt.legend()\n",
    "  \n",
    "#   # plt.tight_layout()\n",
    "#   plt.show()\n",
    "\n",
    "# plot_results(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "# plot_decision_region(large_model, x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ef3d89592ae3e4b31d52cf243fb0d22",
     "grade": false,
     "grade_id": "cell-7ad8cc34d293f12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How do the decision regions differ between the tiny, large and the network you trained? Can you explain why this happens? Relate your answer to the concepts you learned in the first lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e61e4bd20e11dece795ed80558fa0c6f",
     "grade": true,
     "grade_id": "cell-b2b3d4743f149248",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08f25d921ab7040f778a202f5a69b1a7",
     "grade": false,
     "grade_id": "cell-effc356fff18a4b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b8c71783af4d4c94ae59eb356856591",
     "grade": false,
     "grade_id": "cell-b2bbee2adf94b7d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Back to your original model. Once you achieved at least 60% accuracy in the validation set with your main model, we are done with its training. Now we'll evaluate the performance of your classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79420c8cb4af750ed05be42a2c4d528b",
     "grade": false,
     "grade_id": "cell-3f4ab762e2890554",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a28be10f6e90e487934989e8ef58b193",
     "grade": false,
     "grade_id": "cell-64d1008aafb3e518",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "main_model_test_accuracy = 0\n",
    "# YOUR CODE HERE\n",
    "print(f\"Test accuracy: {main_model_test_accuracy:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3523a57a836d53349b06e311c4085fd5",
     "grade": true,
     "grade_id": "cell-8290e0d3e722c85f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert main_model_test_accuracy > 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4437b0233832d6897896b322856771a3",
     "grade": false,
     "grade_id": "cell-75bfc44531d04fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Most likely, you'll get a different (slightly worse) accuracy than the one you got on the validation set. Why is this? Also, why do we need both a test and validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0737ad696946a7d4383701faa264867",
     "grade": true,
     "grade_id": "cell-249cc96508c4de0b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c68e1c93373146e59048b2f847e15e6f",
     "grade": false,
     "grade_id": "cell-8232dd159f8d4887",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Next, compute the confusion matrix of your predictions on the test set and save it as `conf_mat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97c533c440746fc0b1f79274a2ceee25",
     "grade": false,
     "grade_id": "cell-9475a5163acc3249",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5546a1442dec7b94a0696ee69a270bc6",
     "grade": true,
     "grade_id": "cell-d016323f6263c643",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(conf_mat, np.ndarray) or isinstance(\n",
    "    conf_mat, torch.Tensor\n",
    "), \"conf_mat should be a numpy array or torch.Tensor\"\n",
    "assert conf_mat.shape == (3, 3), \"conf_mat should have shape (3, 3), i.e. 3 classes\"\n",
    "assert np.sum(conf_mat) == len(\n",
    "    dataset_test\n",
    "), \"conf_mat should sum up to the number of test samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "742f3a87c97efede0dc19953c1db108b",
     "grade": false,
     "grade_id": "cell-a5e9c635fcf0e14d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What can you conclude from the computed accuracy and confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59b0c3fb373653839818423f7129f074",
     "grade": true,
     "grade_id": "cell-b93435335accba07",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99a1266a62880be4cf1f724ed704ddb2",
     "grade": false,
     "grade_id": "cell-b731aebed710ae07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 6. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca3f2bc4e6ee91e121425e12e58b589d",
     "grade": false,
     "grade_id": "cell-79904bd828487a07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You have now trained and evaluated a neural network for this particular classification task. Can you provide a brief explanation as to how you could use it to decide where to travel, if you're interested in capturing the aforementioned Pokemons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8aeac76ab83248b3e84126f1200e50f",
     "grade": true,
     "grade_id": "cell-79ac9ff2b09a41b5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90283187bfd4f2f6aaf11df18a8e88ba",
     "grade": false,
     "grade_id": "cell-8041a4537430ba53",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Is (are) there any other feature(s) from the original dataset (e.g. hour of the day, pressure, wind speed, population density, etc.) which you think would be valuable to add as an input feature to your classifier to improve its performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92bfb21c07460ec7bf06f56c3acee803",
     "grade": true,
     "grade_id": "cell-c2def6009a95f2a6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e0eb26832febb3a136740004988db6b",
     "grade": false,
     "grade_id": "cell-e9f21e8b0fe33751",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To investigate your hypothesis, plot a histogram of the selected feature(s) for each one of the pokemons we're interested in. For example, if you think pressure and population density are valuable for prediction, plot 6 histograms. 3 of them will be the pressure histograms for each class ('Diglett', 'Seel' and 'Tauros'), and the other 3 will be the population density for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6701fb6bd1284791b9c065777eaaebae",
     "grade": true,
     "grade_id": "cell-b18deaa33c46ec92",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6845805e5d221d5a320d0630b3fa0257",
     "grade": false,
     "grade_id": "cell-f51c8bdfaf2e03f4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What does/do this/these histogram(s) show you? Could it be beneficial to add this/these new feature(s) as input? Explain why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4810811a9300a653185a38c62a0f73c",
     "grade": true,
     "grade_id": "cell-f9522e4fa9c010be",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbf51d6ca2c3eed999d1e41aa1cec7de",
     "grade": false,
     "grade_id": "cell-284bf5af1750b1b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The purpose was this assignment was mostly to make you design a network for classification, using this Pokemon dataset as use case. However, if you want to find those three particular Pokemons, most likely using a network for classification is not the best approach. An alternative would be to perform localization by using regression instead. **Can you state some pros and cons of approach this as a regression problem instead of a classification problem?** (We do not expect very detailed answers, you will pass the assignment as long as you make a reasonable attempt at explaining the pros and cons.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd2cd90f071d43e463aea1a3f0852c49",
     "grade": true,
     "grade_id": "cell-bd241242621ae646",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c457c6ffe7696ea8b13215805bb2fa2c",
     "grade": false,
     "grade_id": "cell-fccdd48334c7c70c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 7. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efe34b502ca4d51a53e7b32992411f97",
     "grade": false,
     "grade_id": "cell-3e8a75d10f4fb404",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Assuming you found useful new features in the last part of this assignment, train a new classifier that uses these featues as well. Did the accuracy on the validation set improve? What's the highest accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d483a1678e8725ea9ba41ce15bb78a64",
     "grade": true,
     "grade_id": "cell-c4a9ddfda9cd7a08",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "39c912f2fd3fdf7d106cc51aff015157ada41dad8144becd8bcb717d81c78182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
